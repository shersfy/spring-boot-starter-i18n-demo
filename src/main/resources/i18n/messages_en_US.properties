#i18n in locale 'ENGLISH'
#配置源和目标
db.title.configResource=Source & destination  
#配置数据表
db.title.configTable=Config table
#设置导入规则
db.title.importRule=Setting rule  
#配置源信息
db.title.sourceInfo=Configure the source information
#资源库类型
db.option.resourceType=resource type
#资源库管理中已保存连接
db.option.savedLink=The connection has been saved in the resource library management    
#新建临时连接
db.option.tmpLink=New temporary connection
#资源库管理
db.title.resourceManage=resource management
#选择数据库连接
db.option.selectDbLink=choose
#请在此选择之前保存的数据库连接名称,若无已保存的数据库连接,请点击新建。                            
db.note.newLink=Please select the saved database connection,if there is no saved connection,please click resource management                      
#数据库类型
db.option.selectDbType=database type
#配置目标信息 
db.title.configTarget=Configure the destination information
#选择目标位置 
db.title.selectTargetType=destination type
#(注: 数据迁移会根据您的选择, 将源数据库的数据复制一份到目标数据库(新建), 或者是将源#数据库数据追加到目标数据库指定的表内(追加), 并不会对源数据库数据造成影响.)
db.note.tableNote=(note: data migration will be based on your selection:copy the data from the source database  to the object database (new) or the source database data will be added to the object database specified table( additional )and no affection to the  source database data)
#个空格
db.text.space= space(s)
#空字符串
db.text.emptyString=empty string
#确认
db.text.confirm=Confirm
#自定义
db.text.customize=customize
#源数据
db.title.srcData=source data
#全选
db.option.allSelt=check all
#字段
db.option.column=column
#条件
db.option.condition=condition
#源表
db.option.srcTbl=source table
#源视图
db.option.srcView=source view
#目标库
db.option.tarDb=target database
#是否为分区表
db.option.isPart=exist partition
#是
db.text.yes=Yes
#否
db.text.no=No
#配置目标位置
db.title.scanSql=Target location                            
#设置过滤条件
db.title.setCondition=Filter conditions
#条件连接
db.title.ddlConn=Conditional connection                            
#当前没有设置条件
db.note.noCondition=No setting condition currently
#设置条件
db.title.confition=Set condition
#过滤源数据(按where条件)
db.option.whereCondition=Filter source data (by where condition)
#导入增量源数据(按字段对比)
db.option.asyncCondition=Import incremental source data (by field comparison)
#设置where条件
db.text.setWhere=Set where sql
#增加条件               
db.btn.addCondition=Increase the condition                   
#自定义sql         
db.option.customSql=customize sql
#若字段内容包含换行符,换行符默认替换为空格。若不启用替换,将导致字段错乱和行数不匹配。您可根据个人需求进行设置。
db.note.replaceNote=If the field contents contain newline character, the newline character will be replaced by a space. If disabled, the field confusion and number of rows will not match. You can set up according to your personal needs.
#启用换行替换
db.option.replaceEnable=Enable replacement
# 换行符替换为:xx
db.option.replaceMent=Replace line break
#共有
db.text.have=total have             
#个表文件          
db.note.tableAmount=table files    
#配置
db.text.config=config                       
#HIVE到HIVE迁移
db.title.hiveMigrate=HIVE to HIVE migration                            
# 请输入关键字
db.note.searchTableTip=Please enter the keyword
#必读
db.text.readMe=readMe
#说明
db.text.state=state
#默认为空格
db.text.defSpace=Deault space
#迁移类型
db.option.transType=transfer type
#迁移库表
db.option.transTbl=transfer table
#迁移视图
db.option.transView=transfer view
#未找到匹配内容
db.text.matchNone=data matched is none
#未检查到数据
db.text.queryNone=data queried is none
#新建分区
db.act.newPart=new partition
#配置分区
db.act.confPart=config partition
#源分区字段
db.text.srcColumn=source partition column
#目标分区字段
db.text.tarColumn=target partition column
#目标分区存在的处理方式
db.option.tarPartDeal=The way when the target partition exists
#是否存在分区
db.title.isExistPart=Is there partition
#配置方式
db.option.confType=config type
#勾选
db.option.select=select
#使用通配符
db.text.usePattern=use pattern
#源文件
db.text.srcFile=source file
#源路径
db.text.srcPath=source path
#通配符
db.text.pattern=pattern
#目标路径
db.option.tarPath=target path
#源
db.title.source=source
#预览
db.act.preview=preview
#匹配方式
db.option.matchType=match way
#非递归
db.text.nonRecursion=non-recursion
#递归
db.text.recursion=recursion
#设置
db.title.setting=setting
#不改变文件位置
db.text.noChangeDir=Do not change the file location
#迁移后删除文件
db.text.transDel=Remove files after migration
#迁移后将文件移动至
db.text.transMoveDir=After migration, moving file to
#请输入通配符(正则表达式),多个表达式用'|'分割)
db.text.enterInfo=Please enter a pattern (regular expression), and multiple expressions are divided by '|'.
#目标
db.text.target=target
#目标文件名
db.option.tarDir=target filename
#使用源文件名
db.text.useSrcFilename=use source filename
#源文件名加上
db.text.srcFilenameAdd=source filename suffix
#导入日期
db.text.importDate=import date
#如
db.text.example=example
#导入时间
db.text.importTime=import time
#若文件重名
db.option.fileNameSame=duplicate
#覆盖
db.text.override=override
#追加
db.text.append=append
#跳过
db.text.ignore=ignore
#重命名自动添加后缀
db.text.fileNameSuffix=Rename automatically add suffixes
#文件匹配情况
db.text.matchFile=Files matched
#未找到匹配的文件,请检查路径和正则表达式
db.text.matchNoneNote=No matching files are found, please check the paths and regular expressions
#设置增量对比字段
db.title.setSynch=Setting incremental compare column
#选择主键
db.option.choosePkName=Select a primary key
#源表字段
db.text.srcTblColumn=source table column
#目标表字段
db.text.tarTblColumn=target table column
#存储格式
db.option.tableType=format
#格式化
db.act.sqlFormat=SQL format
#查询SQL编辑框
db.title.sqlEditor=SQL Editor
#仅显示前5条数据
db.text.querylimit5=Only show the top 5 data
#迁移数据预览结果
db.title.preViewResult=Preview results
#SQL查询
db.option.sqlQuery=SQL query
#字段映射和清洗
db.title.columnsMap=Field mapping and cleaning
#字段映射配置
db.option.columnsConfig=Field mapping configuration
#按字段顺序自动映射
db.option.mapIndex=automatic mapping by field order
#按字段名称自动映射
db.option.mapName=automatic mapping by field name
#清空配置
db.option.emptyMap=empty the configuration
#类型
db.text.type=type
#函数列表
db.title.funcList=Function list
#数据内容处理
db.option.dataFunc=Data content handle
#字符替换
db.text.Replace=replace characters
#替换换行符
db.text.ReplaceEnter=replace line character
#去除空格
db.text.trim=remove the blank space
#字符长度截取
db.text.substr=character length intercept
#剔除字符
db.text.delChar=excluding character
#增加字段
db.option.addColumn=increase column
#增加时间函数
db.text.timestamp=increment time function
#增加常值
db.text.const=increase constant
#默认返回NULL
db.text.null=default returns NULL
#保存整个配置
db.act.saveConfig=save the entire configuration
#函数名称
db.text.funcName=function name
#替换前
db.text.oldChar=old char
#替换后
db.text.newChar=new char
#添加需要替换的字符
db.act.oldChar=add old char
#去除首尾空格
db.option.trimBlank=remove the trailing spaces
#去除所有空格
db.option.trimAll=remove all spaces
#正序
db.text.order=order
#逆序
db.text.reverse=reverse
#个字符开始截取
db.text.charSub=characters begins to intercept
#截取长度
db.text.subLen=intercept length
#不限制
db.text.nolimit=no limit
#选择类型
db.text.selectType=choose type
#时间戳
db.text.stamp=timestamp
#目标数据
db.text.tarData=target data
#待映射列表
db.text.mapTblList=unmapped list
#忽略
db.text.skip=skip
#中断
db.text.interrupt=interrupt
#过滤函数
db.text.filterFunc=filter function
#SQL文件
dump.title.title=SQL file
#管理SQL文件
dump.title.version=Manage SQL file
#请输入备份名关键字进行查询	
dump.option.search=Please enter the keyword of the backup name to query
#备份日期	
dump.title.verDate=backup date
#备份名	
dump.option.verName=backup name
#表数量	
dump.tblNum=table number
#操作日期	
dump.option.opDate=operation date
#上传方式	
dump.upType=upload mode
#上传用户	
dump.option.upUser=upload user
#查看备份SQL文件	
dump.title.verSearch=View backup file
#备份信息	
dump.verInfo=Backup Information
#备份名
dump.option.verNum=backup name
#备份创建时间	
dump.option.verNewTime=Backup creation time
#文件总大小	
dump.option.fileSize=The total file size
#文件数	
dump.option.fileNum=Number of files
#目标类型	
dump.option.tarType=object tpye
#至
dump.text.to=to
#浏览器
dump.text.browser=browser
#客户端
dump.text.client=client
#的基本信息
dump.text.info=basic information
#目标表SQL语句
dump.text.tarSql=The target table SQL statement
#备份数据表	
dump.title.verTbls=Backup data table
#所导入的数据表	
importedTbls=The imported data sheet
#备份地址	
dump.option.bakAddr=Backup address
#只显示前100条记录	
dump.option.show100=Only show the first 100 records
#SQL文件迁移	
dump.title.dumpMigrate=SQL file migration
#上传SQL文件	
dump.title.upDumpFiles=Upload the SQL file
#配置导入路径	
dump.title.importPath=Configure the import path
#添加SQL文件	
dump.option.addDump=Add files
#(注：仅支持insert文本文件内容,及*.sql,*.txt,*.gz,*.zip的格式解析;且一次任务最多同时上传5个SQL文件。)	
dump.note.dumpNote=(Note: only support insert *.sql, *.txt, *.gz, *.zip format resolution, and upload five SQL files in one task)
#批量配置	
dump.title.batchConfig=batch
#对此次上传的SQL文件进行备份	
dump.option.isBak=Backup the SQL file for this upload
#备份后,所上传的SQL文件会额外保存在备份库中,并能在SQL版本管理中查看相关信息及预览其中部分信息内容。	
dump.note.dumpBakNote=After the backup, the uploaded SQL file will be saved in the backup library, and the relevant information and preview some of the information content can be viewed in the SQL version management  
#请选择需要备份的路径	
dump.option.bakPath=Please select the path you want to back up
#提示
dump.option.tip=alert
#您本次的导入操作中,有
dump.text.importAct=Your import operation, there are
#张表在目标位置不存在,系统将默认为您进行建表操作。若新建序列存在相同目标库、目标表则默认新建第一个。	
dump.text.newTblNote=tables  do not exist in the object location, the system will create table by default setting. If the new sequence exists the same object library, object table will creat new one by default
#新建表如下
dump.text.newTblList=The new table as follows
#以下新表创建失败：
dump.text.newTblFail=The following new table creation fails
#导入
dump.text.import=import
#源库名	
dump.option.sourceDb=source database
#目标表名	
dump.option.tarTbl=target table name
#已了解,继续下一步	
dump.note.nextStep=Understand, next step
#取消,我要修改	
dump.note.cancel=Cancel, I want to modify
#所有任务数据配置完毕！
dump.note.configOk=All the task is configured!
#所有目标表验证完毕！
dump.note.tblsNew=All target tables are verified!
#亚马逊S3数据迁移
s3.title.s3Migrate=Amazon S3 data migration
#配置连接参数
s3.title.configLink=Connection parameters
#选择表及配置目标路径
s3.title.configParam=Select the table and configure the target path
#请提供Amazon S3的Access Key（访问密钥）和Secret Key（秘密密钥）
s3.option.key=Please provide Amazon S3's Access Key and Secret Key
#访问密钥ID
s3.option.keyId=access key
#私有访问密钥
s3.option.accKey=private key
#区域
s3.option.region=area
#注意：Amazon S3的文件目前只支持导入到HDFS
s3.note.hdfsNote=Note: Amazon S3 files currently only support import to HDFS
#已选择
s3.option.selected=selected
#所有任务数据配置完毕！
s3.note.configed=all data have been configured
#开始任务
s3.note.start=Start the task
#请求资源为空,请重新加载
s3.info.resEmpty=Request resource is empty, please reload
#请求资源为空
s3.info.resErr=Request resource is empty
#迁移
s3.title.migrate=migrate
#注意：FTP目前只支持导入到HDFS
ftp.note.hdfsNote=Note: FTP currently only supports import to HDFS
#注：根据您的配置,可对目标位置已有的同名文件进行替换。
ftp.note.importNote=Note: according to your configuration, you can replace the same file with the same name in the target location.
#文件夹名
ftp.option.dirTitle=dir name
#返回根目录
ftp.option.returnRoot=return root directory
#本地文件迁移
local.title.localMigrate=Local file migration
#配置目标路径
local.title.tarPath=Destination path
#目标位置
local.title.tarDir=destination type
#添加文件
local.option.addFile=Add files
#(注：最多批量上传5个文件;文件大于500M, 建议使用客户端上传.若目标位置存在跟迁移对象同名的文件，系统默认对迁移成功的文件名增加后缀处理。(例：测试文件_1、测试文件_2))
local.note.limit500=(Note: upload up to 5 files in bulk; If the file is larger than 500M, we recommand you to use the client upload. If there is a file share same name with the migrated file in the targeted folder, the system will add suffix for the migrated file automatically. (E.g. Test file_1, test file _2))
#注：若目标位置存在跟迁移对象同名的文件，系统默认对迁移成功的文件名增加后缀处理。(例：测试文件_1、测试文件_2)
csv.text.hdfsNote=Note: If there is a file share same name with the migrated file in the targeted folder, the system will add suffix for the migrated file automatically. (E.g. Test file_1, test file _2)
#文件大小
local.option.fileSize=file size
#上传
local.option.up=upload
#路径配置
local.option.confPath=config path
#解析
local.option.parse=analysis
#开始导入
local.title.startTask=Start importing
#文件目录
local.title.fileDir=file dir
#文件名
local.title.fileName=file name
#CSV迁移
local.title.csvMigrate=CSV migration
#EXCEL迁移
local.title.excelMigrate=EXCEL migration
#配置目标位置
local.title.configTar=Target location
#(注: 支持CSV和TXT文件, 最大800M, 最多5个文件批量上传)
local.note.limit800=(Note: support CSV and TXT files, no more than 800M and 5 files in one bulk upload)
#数据源
local.title.dataSrc=Source
#导入配置
local.title.importConf=Destination
#HDFS目录
local.title.hdfsDir=HDFS dir
#浏览
local.option.scan=Browse
#从第
local.text.from=From
#行开始导入数据,之前的数据将不做保留
local.note.saveLineNum=line importing data, and the previous data will not be retained
#目标SQL语句
local.option.tarSql=Target SQL statement
#若您填写的表名在目标位置不存在,系统会默认通过SQL语句新建表以导入数据。若建表SQL有误,将影响数据的导入。请您检查目标SQL语句并根据个人需求进行修改。
local.note.newTblNote=If you fill in the table name does not exist in the target position, the system will defaultly by the SQL statement create table to import the data. If the SQL is wrong, will affect the import data. Please check the target SQL statements and modified according to individual demand.
#CSV文件参数
local.title.csvConf=CSV file param
#制表符
local.option.tab=tabs
#分号
local.option.semicolon=semicolon
#逗号
local.option.comma=comma
#空格
local.option.space=space
#其他
local.option.other=other
#(注: 支持后缀为.xls和.xlsx的Excel文件, 最大100M, 最多5个文件批量上传)
local.note.excLimit100=(Note: support Excel file suffix for .xls and .xlsx, up to 100M, up to 5 files in bulk upload)
#库
local.option.db=database
#表
local.option.tbl=table
#分区
local.option.part=partition
#列分隔符
local.option.columnSep=ColumnSep
#文本限定符设置
local.option.endSet=Qualifier settings
#文本限定符
local.option.endChar=Text qualifier
#保留
local.option.stay=retain
#去掉后导入
local.option.quit=remove and import


#客户端任务管理
clientM.title.clientTaskM=Client Task Management
#客户端管理
clientM.title.clientM=Client management
#任务查看
clientM.title.taskView=Task view
#新建任务
clientM.title.newTask=New task
#天调度类型
clientM.option.dayExec=Day scheduling type
#周调度类型
clientM.option.weekExec=Week scheduling type
#月调度类型
clientM.option.monthExec=Monthly scheduling type
#小时调度类型
clientM.option.hourExec=Hourly dispatch type
#定时执行
clientM.option.timing=regular execution
#立即执行
clientM.option.immedi=immediate implementation
#任务说明
clientM.option.taskNote=task description
#调度生效日期
clientM.cronLife=scheduled effective date
#无
clientM.option.none=none
#客户端列表
clientList.title.clientList=Client List
#客户端上传
clientList.title.clientUp=Client management
#下载客户端
clientList.title.downClient=Download client
#同步客户端
clientList.title.synch=Synchronize the client
#本地文件数据轻松管理,自动同步本地资源、文件、SQL文件等至设置好的目标位置,告别频繁上传、文件管理不再愁。
clientList.note.clientDesc=Easily manage local file and data,local resources ,files,SQL files synchronizate automatically to the destination location, no more frequent upload.
#备注
clientList.note.mark=remarks
#一个IP地址只能启动一个客户端
clientList.note.ip2client=A computer can only start a client
#客户端成功部署以后,将进入客户端管理列表
clientList.note.accList=After the client has successfully deployed, it will appear in the client list
#下载新的客户端
clientList.option.downNewClient=Download
#客户端成功部署以后,在客户端列表可以看到您的客户端信息
clientList.note.clientNews=(After the client is successfully deployed, you can see your client information in the client list)
#客户端名
clientList.option.clientName=client name
#服务地址
clientList.option.ip=host
#操作
clientList.option.oper=operation
#任务
clientList.option.task=task
#任务名
clientList.option.taskName=task name
#状态
clientList.option.status=status
#删除
clientList.option.del=delete
#删除客户端后,该客户端所有任务将被停止,并且在客户端列表中清除该客户端所有信息。
clientList.note.delClientNote=After the client is deleted, all tasks of the client will be paused , and all information about the client will be cleared
#确定将该客户端删除？
clientList.note.isDel=Are you sure that the client should be deleted?
#迁移任务首页
index.title.index=Migrate the job home page
#数据迁移
index.title.dataMigrate=Data migrate
#任务管理
index.title.taskManage=Task management
#新建迁移任务
index.title.newTask=New task
#任务布置成功
index.title.succPage=The task was assigned successfully
#您是要
index.text.will=Do you want
#进行查看
index.text.view=to view
#还是前往
index.text.goto=or go to
#全部
index.title.all=All
#本地文件
index.title.local=Local file
#数据库
index.title.db=Database
#第三方平台
index.title.otherPlat=Third-party platform
#其他
index.title.other=Others
#本地到HDFS
index.title.local2Hdfs=Local to HDFS
#亚马逊
index.title.amazon=Amazon
#阿里云
index.title.ali=Aliyun
#返回导航页
common.title.retIndex=Return to navigation page
#aside 数据迁移
common.title.dataMigrate=
#总览
common.title.overview=Overview
#退出
common.title.exit=logout
#语言项
common.option.lang=中文
#权限管理
common.title.auth=User Admin
#数据迁移
common.title.migrate=Data migrate
#数据采集
common.title.collect=
#数据下载
common.title.down=
#迁移任务管理
common.title.migrateM=Task management
#新建迁移任务
common.title.newTask=New task
#任务执行历史记录
common.title.runLog=Execution history
#迁移运行实例
common.title.migrateEg=Running instance
#设置Settings
common.title.setting=Settings
#资源库信息管理
common.title.resM=Resource management
#环境配置
common.title.envir=Environment config
#编辑
common.title.edit=edit
#任务详情
common.option.taskDetail=task detail
#报警服务
common.title.alarmService=Alarm service
#报警历史
common.option.alarmRecord=Alarm records
#报警规则
common.option.alarmSetting=Alarm rules
#流式计算
common.title.pipeline=
#开放API
common.tab.openAPI=
common.title.openAPI=Open API
#Token管理
common.title.token=Token manage
#API说明文档
common.title.api=API docs

#数据库连接管理
dbManage.title.dbLink=Database connection
#Hive连接信息
dbManage.title.hiveLink=Hive connection
#Hdfs连接信息
dbManage.title.hdfsLink=Hdfs connection
#Ftp连接信息
dbManage.title.ftpLink=Ftp connection
#Spark MPP连接信息
dbManage.title.sparkLink=Spark MPP connection
#协议
dbManage.option.protocol=protocol
#文件传输协议
dbManage.option.ftp=File transfer protocol
#新建资源库信息
dbManage.title.newRes=New resource
#资源信息修改
dbManage.title.resEdit=Resource information modification
#基础
dbManage.title.base=Basis
#参数配置
dbManage.title.param=Parameter
#请选择项目
dbManage.title.setApp=Please select project
#资源库类型
dbManage.title.setResType=resource type
#服务地址
dbManage.option.addr=host address
#认证方式
dbManage.title.authType=authentication
#路径
dbManage.option.path=path
#文件路径
dbManage.option.fileName=file path
#用户名
dbManage.option.userName=user name
#连接名
dbManage.option.connName=connection name
#保存
dbManage.option.save=Confirm
#连接
dbManage.option.link=connection 
#代理用户
dbManage.option.proxy=proxy user
#端 口
dbManage.option.port=port
#数据库名
dbManage.option.dbName=database name
#密码
dbManage.option.pwd=password
#所属项目
dbManage.option.belongApp=project
#数据库类型
dbManage.option.dbType=db type
#上传文件
dbManage.title.upFile=Upload files
#上传文件btn
dbManage.button.upFile=upload
#Kerberos域
dbManage.title.kerberos=Kerberos domain
#自定义连接名
dbManage.title.linkName=connection name
#绑定的HDFS连接
dbManage.title.setHdfsLink=bind HDFS
#HIVE代理用户
dbManage.title.hiveUser=HIVE proxy user
#选择功能参数
dbManage.title.setParam=select parameter
#命名参数
dbManage.title.nameParam=named parameter
#值
dbManage.title.val=value
#测试连接
dbManage.title.testConn=Test the connection
#新增一行
dbManage.option.newline=Add newline
#请输入服务地址
dbManage.enter.host=Enter host
#请输入数据库名
dbManage.enter.dbName=Enter database name
#请输入用户名
dbManage.enter.userName=Enter username
#请输入密码
dbManage.enter.password=Enter password
#请自定义连接名,默认为"服务地址_端口_数据库名"
dbManage.enter.connName=Enter connection name, 'host_port_dbName'
#请填写与上传keytab文件对应的keberos域
dbManage.enter.principal=Enter kerberos domain matching keytab file
#请自定义连接名,默认为："hdfs_认证方式"
dbManage.enter.hdfsConnName=Enter connection name, 'hdfs_authentication'
#请输入端口
dbManage.enter.port=Enter port
#请输入代理用户
dbManage.enter.hiveUserName=Enter proxy username
#请自定义连接名,默认为："协议_服务地址_端口"
dbManage.enter.hiveConnName=Enter connection name, 'protocol_host_port'

#同一项目内的成员将共同管理项目内的资源库连接
dbManage.tip.app=Members within the same project will jointly manage the  connection within the project
#资源库包含数据库、Hive、HDFS、FTP、Spark MPP类型。创建Hive连接时,请确保已创建同一集群的HDFS连接
dbManage.tip.type=The resource library contains database, Hive, HDFS, FTP, Spark MPP types. When creating a Hive connection, please make sure that you have created an HDFS connection for the same cluster
#连接名将作为数据源的标识
dbManage.tip.connName=The connection name is the identity of the data source
#选择认证方式时,请确保要添加的集群已集成相对应的权限管理
dbManage.tip.authType=When you select the authentication method, please make sure that the cluster you want to add is integrated with the corresponding rights management
#core-site.xml是Hadoop集群的配置文件,通常在Hadoop安装目录内,如路径../conf/core-site.xml,请上传
dbManage.tip.coreSite="core-site.xml" is a configuration file of the hadoop cluster, generally exists in the Hadoop installation directory, such as the path "../conf/core-site.xml", please upload
#hdfs-site.xml是Hadoop集群的配置文件,通常在Hadoop安装目录内,如路径../conf/hdfs-site.xml,请上传
dbManage.tip.hdfsSite="hdfs-site.xml"  is a configuration file of the hadoop cluster, generally exists in the Hadoop installation directory, such as the path "../conf/hdfs-site.xml", please upload
#HDFS用户名与该集群的HDFS目录权限有关
dbManage.tip.hdfsUser=The HDFS user name is related to the HDFS directory permissions of the cluster
#krb5.conf是hadoop集群kerberos认证方式的配置文件,通常所在位置如/etc/krb5.conf,请上传
dbManage.tip.krb="krb5.conf" is a configuration file of hadoop cluster with kerberos authentication mode , generally located such as "/etc/krb5.conf", please upload
#keytab文件是hadoop集群kerberos认证方式的配置文件,通常所在位置如/etc/security/keytabs/datahub.keytab,请上传
dbManage.tip.keytab="keytab file" is a configuration file of hadoop cluster with kerberos authentication mode , generally located such as "/etc/security/keytabs/datahub.keytab", please upload
#hadoop集群的Kerberos域名,如datahub@LEAP.COM
dbManage.tip.kerberos=Kerberos domain name of  hadoop cluster, such as "datahub@LEAP.COM"
#hive连接需绑定同一集群的HDFS连接
dbManage.tip.bindHdfs=The hive connection needs to bind the HDFS connection of the same cluster
#服务地址,如demo1.test.com
dbManage.tip.host=service address, such as demo1.test.com
#连接的端口
dbManage.tip.port=connection port
#hive库表权限依赖于此代理用户,请在集群内创建相应的目录,并赋予此代理用户。如代理用户为“demo”,请在集群内创建目录“/user/demo”,且此目录属于用户“demo”
dbManage.tip.proxy=The hive database and table permissions depend on this proxy user. Please create the appropriate directory in the cluster and give to this proxy user. For example, the hive proxy user is "demo". Please create the directory "/user/demo" in the cluster HDFS, and this directory belongs to the user "demo"
#连接URL会按照服务地址等填写信息自动生成,若集群为kerberos认证方式,请对连接串内principal的值进行修改。principal值是hive配置文件hive-site.xml中hive.metastore.kerberos.principal值
dbManage.tip.url=The connection URL would be automatically generated according to the information you filled in. If the cluster is kerberos authentication mode, please modify the value of "principal" in the connection string. The "principal" is "hive.metastore.kerberos.principal" which in the hive configuration file "hive-site.xml"
#服务模式
dbManage.option.mode=service mode
#普通模式
dbManage.option.normalMode=normal mode
#HA模式
dbManage.option.haMode=HA mode
#ZK节点
dbManage.option.zk=ZK node
#添加
dbManage.option.add=add

#当前集成了hadoop集群的两种认证方式（sentry,kerberos）。选择认证方式时,请确保部署集群已集成相对应的权限管理
envir.tip.sentry=Two authentication methods (sentry, kerberos) have been integrated with hadoop clusters. When you select the authentication method, please make sure that the deployed cluster was integrated with the corresponding rights management
#core-site.xml为hadoop集群的配置文件,通常所在位置为：/etc/hadoop/conf/core-site.xml
envir.tip.coreSite=core-site.xml is a configuration file of the hadoop cluster,  the general location is "/etc/hadoop/conf/core-site.xml"
#hdfs-site.xml为hadoop集群的配置文件,通常所在位置为：/etc/hadoop/conf/hdfs-site.xml
envir.tip.hdfsSite=hdfs-site.xml is a configuration file of the hadoop cluster, the general location is "/etc/hadoop/conf/hdfs-site.xml"
#krb5.conf为hadoop集群kerberos认证方式的配置文件,通常所在位置为：/etc/krb5.conf
envir.tip.krb5Path=krb5.conf is a configuration file of the hadoop cluster with kerberos authentication  mode , the general location is "/etc/krb5.conf"
#部署集群为kerberos认证方式时,通常使用datahub.keytab进行配置,通常所在位置为：/etc/security/keytabs/datahub.keytab。使用datahub.keytab前,请确保/user/datahub目录已存在,并此目录属于datahub用户
envir.tip.keytabPath=When the deployed  cluster is kerberos authentication mode, it generally configured with "datahub.keytab", the general location is "/etc/security/keytabs/datahub.keytab". Before using datahub.keytab, please make sure that the "/user/datahub" directory exsits and  the directory belongs to the user "datahub"
#hadoop集群Kerberos认证方式下,keytab的principal值,如datahub@LEAP.COM
envir.tip.kerberos=In the hadoop cluster Kerberos authentication method, the  principal value of  keytab ,such as datahub@LEAP.COM
#datahub所部署集群的HDFS连接名默认为LEAP_SYSTEM_HDFS
envir.tip.hdfsName=The HDFS connection name of the datahub deployed cluster is "LEAP_SYSTEM_HDFS" by  default
#datahub所部署集群的Hive jdbc连接URL
envir.tip.hid=The Hive jdbc connection URL of the  datahub deployed cluster
#代理用户与Hive库表权限有关
envir.tip.proxy=The proxy user is associated with the Hive database and table permissions
#datahub所部署集群的HIVE连接名默认为LEAP_SYSTEM_HIVE
envir.tip.hiveName=The Hive connection name of the datahub deployed cluster is "LEAP_SYSTEM_HIVE" by  default

#项目环境配置
envir.tab.envir=Project environment configuration
#运营商服务配置
envir.tab.sms=SMS service configuration
#流式计算配置
envir.tab.streamEntry=Streamsets configuration
#测试
envir.act.test=Test
#接收号码参数名
envir.option.phoneNumberKey=phone number key
#短信内容参数名
envir.option.contentKey=content key
#其他参数
envir.option.params=other params
#添加参数
envir.option.addParam=add param
#输入手机号
envir.text.enterPhone=enter phone
#案例
envir.text.case=Case

#迁移任务查询
manage.title.taskSearch=Task query
#任务ID
manage.title.taskId=task ID
#任务名称
manage.title.taskName=task name
#请输入任务名称
manage.text.inputTaskName=Please enter taskName
#任务状态
manage.title.taskStatus=status
#所有
manage.title.all=all
#执行中
manage.title.execing=in execution
#结束
manage.title.end=finished
#暂停
manage.title.pause=pause
#任务执行时间
manage.title.execTime=execution date
#创建人
manage.title.owner=creator
#我创建的
manage.title.myPro=mine
#所属项目
manage.title.inApp=project
#全部项目
manage.title.allPro=all items
#查询
manage.title.search=Query
#重置
manage.title.reset=Reset
#任务描述
manage.title.taskDesc=task details
#源类型
manage.title.srcType=source type
#调度类型
manage.title.cronType=scheduling type
#任务创建时间
manage.title.createTime=create time
#暂无数据
manage.title.noData=No data
#停用
manage.title.stop=disabled
#激活
manage.title.enable=activation
#修改
manage.title.update=modify
#复制
manage.title.copy=copy
conf.btn.copy=Copy
#运行记录
manage.title.runLog=run record
#执行历史查询
manage.title.hisSearch=Execution history query
#运行中
manage.title.running=in operation
#成功
manage.title.succ=success
#失败
manage.title.fail=fail
#执行日期
manage.title.execDate=execution date
#开始时间
manage.title.startTime=start time
#结束时间
manage.title.endTime=end time
#运行时长
manage.title.runTime=take time
#日志
manage.title.log=log
#下载失败明细
manage.act.failItems=Download failure details
#客户端任务
client.title.clientTask=Client tasks
#配置源
client.title.confSrc=Config source
#配置导入路径
client.title.confUpPath=Config destination path
#导入规则设置
client.title.confRule=Import rule
#指定源文件路径
client.option.srcPath=source file path
#填写源文件所在路径
client.title.inputSrcPath=Enter the file path
#是否上传子目录文件
client.title.isUpdir=Whether to upload a subdirectory file
#过滤上传文件
client.title.filterFile=filter files
#勾选此项后，可对指定需要上传的源位置进行文件类型过滤。
client.note.filterTip=After selected, you can filter the file type for the source location that you want to upload.
#文件类型过滤
client.option.filterType=file type filtering
#仅上传指定类型文件
client.title.upType=Upload only the specified type file
#不上传指定类型文件
client.title.noUpType=Do not upload a specified type file
#文件类型选择
client.option.selectType=file type selection
#其他类型
client.title.otherType=Other types
#若要过滤的文件类型不在以上备选框内,请在下面填写文件类型的正则表达式并以“|”隔开,例如：.+\.sh|.+\.xml
client.note.filterNote=To filter the file type is not in the above box, please fill in the following regular expression of the file type and separated by "|", for example: .+\\.sh|.+\\.xml
#已上传文件识别设置
client.title.upedReg=Uploaded file recognition settings
#客户端上传任务需要对已上传过的文件进行识别,以免进行重复上传,您可以选择将已上传过的文件自动增加后缀标识,或者移动到其他的文件夹,系统会自动根据您的设置来完成操作。
client.note.upedRegNote=The client need to identify the files have been uploaded to avoid repeatedly upload.You can choose to add the uploaded file automatically to the suffix logo or move to another folder. The system will automatically complete the file according to your settings. operating.
#选择处理方式
client.option.dealType=select type
#删除本地已上传的文件
client.option.delUped=Delete a locally uploaded file
#为已上传文件添加后缀
client.option.addSuff=Add a suffix to the uploaded file
#将已上传的文件移动到
client.option.upedMove=Move the uploaded file to
#取消
client.option.cancel=Cancel
#上一步
client.option.prevStep=Previous
#下一步
client.option.nextStep=Next
#选择连接
client.option.selectLink=select connection
#填写目标路径
client.option.tarDir=destination path
#选择目标数据库
client.option.tarDb=select database
#选择目标数据表
client.option.tarTbl=select table
#选择分区
client.option.part=select partition
#若指定的目标表与待上传的表重名,则
client.note.sameNameTbl=If the specified target table is heavily identified with the table to be uploaded, then:
#覆盖目标表数据
client.option.override=Overriding the target table data
#向目标表追加数据
client.option.append=Append data to the target table
#配置任务信息
client.title.confTask=Configure task information
#任务名称长度不能超过30个字符。
client.info.nameLen=the length of the task name can not exceed 30 characters
#最长255个字符
client.info.maxLen=maximum 255 characters
#执行一次
client.option.once=Run once
#重复执行
client.option.regExec=repeatedly execute
#定时执行
client.option.timing=scheduled execution
#日期
client.option.date=date
#时间
client.option.time=time
#周期
client.option.cycle=cycle
#执行时间
client.option.execTime=execute time
#每小时
client.option.perHour=per hour
#每天
client.option.perDay=every day
#每周
client.option.perWeek=weekly
#每月
client.option.perMonth=per month
#第
client.text.first=the first
#号
client.option.dayNum=day
#分钟
client.option.minNum=minute
#管理
client.option.manage=Manage
#点
client.text.hour=hour
#星期一
client.option.Mon=Monday
#星期二
client.option.Tue=Tuesday
#星期三
client.option.wed=Wednesday
#星期四
client.option.thu=Thursday
#星期五
client.option.fri=Friday
#星期六
client.option.sat=Saturday
#星期日
client.option.sun=Sunday
#每月最后一天
client.option.lastDay=The last day of the month
#生效时间
client.option.effectTime=Effective time
#无结束时间
client.option.Endless=No end time
#摘要
client.option.summary=Summary
#说明
client.option.note=Description
#根据完整的参数,自动生成说明
client.note.note=create description automatically base on the complete parameters
#立即执行
client.option.Immed=Immediate implementation
#开始导入
client.option.startImport=Submit
#HDFS配置路径
client.option.hdfsPath=HDFS configuration path
#确定
client.option.sure=Confirm
#请确保上传文件和目标表分隔符一致
client.text.columnTip=Make sure the upload file is consistent with the target table delimiter
#文件过滤设置
client.option.fileFilter=filter file setting
#不过滤上传文件
client.option.filterNone=no filtering file
#迁移设置
client.option.transSetting=migrate setting
#源文件中列分隔符
client.option.srcColumnSep=source delimiter
#一次性导入
cron.title.once=Run once
#周期性自动导入
cron.title.cycle=repeatedly execute
#关闭日期框
cron.option.closeCal=Closing date box
#AWZ regions
us-gov-west-1=★ US East (Government)
ap-northeast-1=Asia Pacific (Tokyo)
ap-northeast-2=Asia Pacific (Seoul)
ap-south-1=Asia Pacific (Mumbai)
ap-southeast-1=Asia Pacific (Singapore)
ap-southeast-2=Asia Pacific (Sydney)
ca-central-1=Canada (Central)
eu-central-1=EU (Frankfurt)
eu-west-1=EU (Ireland)
eu-west-2=EU (London)
sa-east-1=South America (São Paulo)
us-east-1=US East (N. Virginia)
us-east-2=US East (Ohio)
us-west-1=US West (N. California)
us-west-2=US West (Oregon)
cn-north-1=★  China North (Beijing)

# 提示信息 start
# MSG.TypeCode.ErrorCode
# MSG.TypeCode.InfoCode
MSG.T0000.E000000=System Internal Error
MSG.T0000.E000001=System Internal Error: System internal error, please contact administrator
MSG.T0000.E000002=System Internal Error: Unknown host exception %s
MSG.T0001.E000000=Invalid value
MSG.T0001.E000001=Invalid value: %s cannot be empty
MSG.T0001.E000002=Invalid value: %s cannot be less than 0
MSG.T0001.E000003=Invalid value: param %s format error
MSG.T0001.E000004=Invalid value: param size not equal: param %s element size is %s, and param %s element size is %s not equal
MSG.T0001.E000005=Invalid value: param %s size  cannot be empty
MSG.T0001.E000006=Invalid value: param %s max length is %s, and min length is %s
MSG.T0001.E000007=Invalid value: param %s only be digital
MSG.T0001.E000008=Invalid value: param %s min value is '%s', but input '%s'
MSG.T0001.E000009=Invalid value: param %s max value is '%s', but input '%s'
MSG.T0001.E000010=Invalid value: %s AND %s cannot be empty
MSG.T0001.E000011=Invalid value: Hive configures unbinds effective HDFS connection, please check the Hive configuration
MSG.T0001.E000012=Invalid value: param %s length not equal %s
MSG.T0001.E000013=Invalid value: more than max support: the job's data source table count(%s) more than max support, max=%s, current=%s
MSG.T0001.E000014=Invalid value: param %s invalid value, it is makes up of letters, digital and underscores
MSG.T0001.E000015=Invalid value: param %s invalid value, the syntax of building statement is error, it starts with CREATE \n%s
MSG.T0001.E000016=Invalid value: param %s invalid value, the syntax of building statement is error, error near CREATE \n%s
MSG.T0001.E000017=Invalid value: param %s invalid value, the syntax of building statement is error, error near TABLE \n%s
MSG.T0001.E000018=Invalid value: param %s invalid value, the entered name of table [%s] does not match the DDL table name [%s]
MSG.T0001.E000019=Invalid value: param %s invalid value, regular expression format error '%s'
MSG.T0001.E000020=Invalid value: param %s invalid value, amazon S3 region not exist '%s'
MSG.T0001.E000021=Invalid value: parameters are not consistent, %s and %s are not consistent, %s='%s', url='%s'
MSG.T0002.E000000=Environment Error
MSG.T0002.E000001=Environment Error: LEAP system hive URL upgrade, please configure the latest URL information\nnew=%s\nold=%s
MSG.T0002.E000002=Environment Error: LEAP Hadoop kerberos authenticate failed, please check configuration
MSG.T0002.E000003=Environment Error: file not exists %s
MSG.T0002.E000004=Environment Error: krb5.conf file content is invalid, the file exist and size not more than 10M, and includes content "[libdefaults]", "[realms]", "[domain_realm]"\n%s
MSG.T0002.E000005=Environment Error: LEAP HDFS unauthorized the directory [%s], please create a directory first
MSG.T0002.E000006=Environment Error: Kerberos authenticate mode, Hive URL missing principal\n%s
MSG.T0002.E000007=Environment Error: Hive JDBC URL HA mode missing parameter "namespace.hive"
MSG.T0002.E000008=Environment Error: URL format error: %s
MSG.T0002.E000009=Environment Error: Save SMS information error, please check parameters
MSG.T0002.E000010=Environment Error: send message error, please check parameters
MSG.T0003.E000000=DB connector error
MSG.T0003.E000001=DB connector error: connector name %s already exists, please change it to other values
MSG.T0003.E000002=DB connector error: username or password mistake
MSG.T0003.E000003=DB connector error: Unknown database %s
MSG.T0003.E000004=DB connector error: By Port [%s] connect [%s] fail. error: connection refused
MSG.T0003.E000005=DB connector error: not support database type %s
MSG.T0003.E000006=DB connector error: invalid JDBC url format '%s' connect to %s
MSG.T0004.E000000=Create table error
MSG.T0004.E000001=Create table error: create table error\n%s
MSG.T0004.E000002=Create table error: No privilege "create" table for database %s
MSG.T0004.E000003=Create table error: size not equals: result list size %s, and target tables size %s
MSG.T0004.E000004=Create table error: 
MSG.T0004.E000005=Create table error: 
MSG.T0005.E000000=Get databases error
MSG.T0005.E000001=Get databases error: 
MSG.T0005.E000002=Get databases error: 
MSG.T0005.E000003=Get databases error: 
MSG.T0005.E000004=Get databases error: 
MSG.T0005.E000005=Get databases error: 
MSG.T0006.E000000=Get tables error
MSG.T0006.E000001=Get tables error: 
MSG.T0006.E000002=Get tables error: 
MSG.T0006.E000003=Get tables error: 
MSG.T0006.E000004=Get tables error: 
MSG.T0006.E000005=Get tables error: 
MSG.T0007.E000000=Get columns error
MSG.T0007.E000001=Get columns error: 
MSG.T0007.E000002=Get columns error: 
MSG.T0007.E000003=Get columns error: 
MSG.T0007.E000004=Get columns error: 
MSG.T0007.E000005=Get columns error: 
MSG.T0008.E000000=Get DDL error
MSG.T0008.E000001=Get DDL error: origin table %s, target table %s
MSG.T0008.E000002=Get DDL error: size not equals: origin tables size %s, and target tables size %s
MSG.T0008.E000003=Get DDL error: 
MSG.T0008.E000004=Get DDL error: 
MSG.T0008.E000005=Get DDL error: 
MSG.T0008.E000006=Get DDL error: 
MSG.T0009.E000000=Get partitions error
MSG.T0009.E000001=Get partitions error: table not exist %s
MSG.T0009.E000002=Get partitions error: 
MSG.T0009.E000003=Get partitions error: 
MSG.T0009.E000004=Get partitions error: 
MSG.T0009.E000005=Get partitions error: 
MSG.T0010.E000000=Find records error
MSG.T0010.E000001=Find records error: 
MSG.T0011.E000000=Hive connector error
MSG.T0011.E000001=Hive connector error: 
MSG.T0011.E000002=Hive connector error: 
MSG.T0011.E000003=Hive connector error: 
MSG.T0011.E000004=Hive connector error: 
MSG.T0011.E000005=Hive connector error: 
MSG.T0012.E000000=SparkMPP connector error
MSG.T0012.E000001=SparkMPP connector error: 
MSG.T0012.E000002=SparkMPP connector error: 
MSG.T0012.E000003=SparkMPP connector error: 
MSG.T0012.E000004=SparkMPP connector error: 
MSG.T0012.E000005=SparkMPP connector error: 
MSG.T0013.E000000=HDFS connector error
MSG.T0013.E000001=HDFS connector error: connector name %s already exists, please change it to other values
MSG.T0013.E000002=HDFS connector error: krb5.conf file content is invalid, the file exist and size not more than 10M, and includes content "[libdefaults]", "[realms]", "[domain_realm]"\n%s
MSG.T0013.E000003=HDFS connector error: test connect failed %s
MSG.T0013.E000004=HDFS connector error: insert into table "HdfsInfo" object failed
MSG.T0013.E000005=HDFS connector error: Permission denied: user=%s, access=READ_WRITE, owner=%s, path="%s"
MSG.T0013.E000006=HDFS connector error: 
MSG.T0014.E000000=FTP connector error
MSG.T0014.E000001=FTP connector error: list ftp files error
MSG.T0014.E000002=FTP connector error: connector name %s already exists, please change it to other values
MSG.T0014.E000003=FTP connector error: 
MSG.T0014.E000004=FTP connector error: 
MSG.T0014.E000005=FTP connector error: 
MSG.T0015.E000000=S3 connector error
MSG.T0015.E000001=S3 connector error:
MSG.T0016.E000000=Datahub Client error
MSG.T0016.E000001=Datahub Client error: The datahub client is not stated , please start the datahub client [%s]
MSG.T0016.E000002=Datahub Client error: 
MSG.T0016.E000003=Datahub Client error: 
MSG.T0016.E000004=Datahub Client error: 
MSG.T0016.E000005=Datahub Client error: 
MSG.T0017.E000000=Same cluster check error
MSG.T0017.E000001=Same cluster check error: The hdfs user "%s" no privilege check hadoop cluster information, please confirm the hive and its binding HDFS is the same cluster
MSG.T0017.E000002=Same cluster check error: The binding HDFS connection information [%s] and the Hive connection information may not be the same cluster, please check the configuration\nHive access address %s not in hadoop cluster list [\n%s]
MSG.T0017.E000003=Same cluster check error: 
MSG.T0017.E000004=Same cluster check error: 
MSG.T0017.E000005=Same cluster check error: 
MSG.T0018.I000001=Record Not Exist: The %s record not exist [id=%s]
MSG.T0018.E000000=CSV file upload error
MSG.T0018.E000001=CSV file upload error: None of the file end with "*.txt|*.csv" in uploaded files
MSG.T0018.E000002=CSV file upload error: unsupported file format [*.%s]
MSG.T0018.E000003=CSV file upload error: 
MSG.T0018.E000004=CSV file upload error: 
MSG.T0018.E000005=CSV file upload error: 
MSG.T0019.E000001=File Not Exist: file not exist %s
MSG.T0020.E000000=Get separator error
MSG.T0021.E000000=SQL file parse error
MSG.T0021.E000001=SQL file parse error: SQL file parsed content is empty
MSG.T0022.E000000=Upload Error
MSG.T0022.E000001=Upload Error: load hdfs file to hive table failed %s
MSG.T0022.E000002=Upload Error: upload file failed, received file size %s and actual file size %s not equal
MSG.T0023.E000000=File Merge Error
MSG.T0023.E000001=File Merge Error: file md5 check failed, at client md5=%s, at server md5=%s
MSG.T0024.E000000=Login Error
MSG.T0024.E000001=Login Error: User not exist
MSG.T0024.E000002=Login Error: Password mistake
MSG.T0025.E000000=Reset Error
MSG.T0025.E000001=Reset Error: User not exist
MSG.T0025.E000002=Reset Error: Password cann't empty
MSG.T0026.E000000=Register Error
MSG.T0026.E000001=Register Error: 
MSG.T0027.E000000=Save Job Error
MSG.T0027.E000001=Save Job Error: The job is running, please go to Execution History and stop it [%s]
MSG.T0027.E000002=Save Job Error: The job is out of date \ncron=%s, \nEffective time %s ~ %s
MSG.T0027.E000003=Save Job Error: add a job to scheduler failed [%s]
MSG.T0027.E000004=Save Job Error: file not exist %s
MSG.T0027.E000005=Save Job Error: not support target type %s
MSG.T0027.E000006=Save Job Error: 
MSG.T0027.E000007=Save Job Error: 
MSG.T0027.E000008=Save Job Error: 
MSG.T0027.E000009=Save Job Error: 
MSG.T0027.E000010=Save Job Error: 
MSG.T0028.E000000=Scheduling Job Error
MSG.T0028.E000001=Scheduling Job Error: 
MSG.T0029.I000001=Disable Job: The job has been disabled [%s]
MSG.T0029.E000000=Disable Job Error
MSG.T0029.E000001=Disable Job Error: The job is running, please go to Execution History and stop it [%s]
MSG.T0030.I000001=Enable Job: The job has been enabled [%s]
MSG.T0030.E000000=Enable Job Error
MSG.T0030.E000001=Enable Job Error: 
MSG.T0031.E000000=Authenticate Error
MSG.T0031.E000001=Authenticate Error: You are not a member of the project and the operation is invalid
MSG.T0031.E000002=Authenticate Error: You are not the system administrator, the operation is invalid
MSG.T0031.E000003=Authenticate Error: request was rejected, only accept "post" request
MSG.T0031.E000004=Authenticate Error: Permission denied, file or directory can not be deleted, %s not starts with %s
MSG.T0031.E000005=Authenticate Error: Permission denied, directory can not be deleted %s
MSG.T0032.E000000=Delete Error
MSG.T0032.E000001=Delete Error: LEAP system configuration information cann't to delete
MSG.T0032.E000002=Delete Error: 
MSG.T0033.E000000=Download Error
MSG.T0033.E000001=Download Error:
MSG.T0034.E000000=Save Alarm Rule Error
MSG.T0035.E000000=Excel file upload error
MSG.T0035.E000001=Excel file upload error: 
# 提示信息 end

# API提示信息 start
API.1001=token not exist
API.1002=token disabled
API.1003=token expired, deadline %s
API.1004=created token user information error
API.1005=permission denied, token no permission to access
API.1006=not opened API '%s'
API.1007=continuous call api times too much, please wait for %s seconds retry
API.1008=token invalid, please login datahub web page refresh token, may be token user have been removed from project '%s'
# API提示信息 end

#短信API测试内容
sms.test.content=Datahub SMS API prepare successful, please ignore if not your operation
#任务基本信息
conf.title.basicInfo=Task basic information
#信息
conf.option.info=information
#项
conf.option.item=item
#导入规则
conf.option.import=import rule
#源信息
conf.title.sourceInfo=Source Information
#目标信息
conf.title.tarInfo=Target information
#迁移详情
conf.title.confDetail=Migrate detail
#查看SQL
conf.act.viewSql=view SQL
#查看任务执行记录
conf.button.viewLog=View task log
#查看
conf.act.view=view
#目标分区
conf.title.tarPart=target partition
#源分区
conf.title.srcPart=source partition
#方式
conf.option.type=type
#自定义分区
conf.option.customPart=custom partition
#按源分区同步
conf.option.partSynch=Synchronize with the source partition
#若目标分区存在
conf.option.tarExist=If the target partition exist
#覆盖目标分区数据
conf.text.overridePart=Override the target partition data
#向目标分区追加源分区数据
conf.text.appendPart=Append source partition data to the target partition
#不要迁移
conf.text.ignorePart=Don't transfer

#迁移任务报警
alarm.title.transAlarm=Migration task alarm
#客户端任务报警
alarm.title.clientAlarm=Client task alarm
#客户端状态
alarm.option.clientStatus=client status
#客户端任务
alarm.option.clientTask=client task

#迁移任务
rule.title.transRule=Transfer task
#客户端任务
rule.title.clientRule=Client task
#新建规则
rule.act.newRule=New rule
#新建报警规则
rule.title.newRule=New alarm rule
#修改报警规则
rule.title.editRule=Edit alarm rule
#报警类型
rule.option.ruleType=alarm type
#请选择
rule.option.tipSelect=please select
#报警规划
rule.option.ruleSetting=alarm program
#任务失败
rule.type.taskFail=task fail
#客户端离线
rule.option.clientOffline=client offline
#不限制报警次数
rule.option.alarmNolimit=alarm no limit
#每小时报警一次
rule.option.alarmPerhour=alarm per hour
#每3小时报警一次
rule.option.alarmPer3hour=alarm per 3 hours
#每天报警一次
rule.option.alarmPerday=alarm per day
#持续
rule.text.persist=persist
#个周期
rule.text.cycle=cycles
#报警内容
rule.option.alarmContent=alarm content
#报警接收方
rule.option.alarmUser=alarm receiver
#短信
rule.option.sms=SMS
#请输入联系人手机号，多个手机号用,隔开
rule.text.notifyUser=Please enter phone number, multiple phone number separate by ','
#返回顶部
interface.act.toTop=return top
#每个项目下每个用户对应一个独立的Token，用于API接口调用的验证，刷新操作可以更新Token。
token.text.tokenDesc=Each user corresponds to a separate Token for each project, which is used for authentication of API interface calls, and the refresh operation can update the Token.
