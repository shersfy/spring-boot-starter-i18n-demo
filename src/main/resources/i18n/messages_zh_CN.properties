#i18n in locale 'CHINESE'
#配置源和目标
db.title.configResource=配置源和目标  
#配置数据表
db.title.configTable=配置数据表  
#设置导入规则
db.title.importRule=设置导入规则      
#配置源信息
db.title.sourceInfo=配置源信息                            
#资源库类型
db.option.resourceType=资源库类型                         
#资源库管理中已保存连接   
db.option.savedLink=资源库管理中已保存连接                  
#新建临时连接
db.option.tmpLink=新建临时连接
#资源库管理
db.title.resourceManage=资源库管理                           
#选择数据库连接 
db.option.selectDbLink=选择数据库连接               
#请在此选择之前保存的数据库连接名称,若无已保存的数据库连接,请点击新建。             
db.note.newLink=请在此选择之前保存的数据库连接名称,若无已保存的数据库连接,请点击进入资源库管理.   
#数据库类型
db.option.selectDbType=数据库类型
#配置目标信息  
db.title.configTarget=配置目标信息                           
#选择目标位置
db.title.selectTargetType=选择目标位置                    
#(注: 数据迁移会根据您的选择, 将源数据库的数据复制一份到目标数据库(新建), 或者是将源数据库数据追加到目标数据库指定的表内(追加), 并不会对源数据库数据造成影响.)        
db.note.tableNote=(注: 数据迁移会根据您的选择, 将源数据库的数据复制一份到目标数据库(新建), 或者是将源数据库数据追加到目标数据库指定的表内(追加), 并不会对源数据库数据造成影响.)                           
#个空格
db.text.space=个空格
#空字符串
db.text.emptyString=空字符串
#确认
db.text.confirm=确认
#自定义
db.text.customize=自定义
#源数据
db.title.srcData=源数据
#全选
db.option.allSelt=全选
#字段
db.option.column=字段
#条件
db.option.condition=条件
#源表
db.option.srcTbl=源表
#源视图
db.option.srcView=源视图
#目标库
db.option.tarDb=目标库
#是否为分区表
db.option.isPart=是否为分区表
#是
db.text.yes=是
#否
db.text.no=否
#配置目标位置 
db.title.scanSql=配置目标位置                           
#设置过滤条件
db.title.setCondition=设置过滤条件
#条件连接
db.title.ddlConn=条件连接                            
#当前没有设置条件
db.note.noCondition=当前没有设置条件    
#设置条件
db.title.confition=设置条件
#过滤源数据(按where条件)
db.option.whereCondition=过滤源数据(按where条件)
#导入增量源数据(按字段对比)
db.option.asyncCondition=导入增量源数据(按字段对比)
#设置where条件
db.text.setWhere=设置where条件          
#增加条件
db.btn.addCondition=增加条件                            
#自定义sql
db.option.customSql=自定义sql                            
#若字段内容包含换行符,换行符默认替换为空格。若不启用替换,将导致字段错乱和行数不匹配。您可根据个人需求进行设置。
db.note.replaceNote=若字段内容包含换行符,换行符默认替换为空格。若不启用替换,将导致字段错乱和行数不匹配。您可根据个人需求进行设置。
#启用换行替换
db.option.replaceEnable=启用换行替换                         
# 换行符替换为:xx   
db.option.replaceMent=换行符替换为                          
#共有
db.text.have=共有            
#个表文件          
db.note.tableAmount=个表文件
#配置
db.text.config=配置                   
#HIVE到HIVE迁移
db.title.hiveMigrate=HIVE到HIVE迁移                 
# 请输入关键字
db.note.searchTableTip=请输入关键字
#必读
db.text.readMe=必读
#说明
db.text.state=说明
#默认为空格
db.text.defSpace=默认为空格
#迁移类型
db.option.transType=迁移类型
#迁移库表
db.option.transTbl=迁移库表
#迁移视图
db.option.transView=迁移视图
#未找到匹配内容
db.text.matchNone=未找到匹配内容
#未检查到数据
db.text.queryNone=未检查到数据
#新建分区
db.act.newPart=新建分区
#配置分区
db.act.confPart=配置分区
#源分区字段
db.text.srcColumn=源分区字段
#目标分区字段
db.text.tarColumn=目标分区字段
#目标分区存在的处理方式
db.option.tarPartDeal=目标分区存在的处理方式
#是否存在分区
db.title.isExistPart=是否存在分区
#配置方式
db.option.confType=配置方式
#勾选
db.option.select=勾选
#使用通配符
db.text.usePattern=使用通配符
#源文件
db.text.srcFile=源文件
#源路径
db.text.srcPath=源路径
#通配符
db.text.pattern=通配符
#目标路径
db.option.tarPath=目标路径
#源
db.title.source=源
#预览
db.act.preview=预览
#匹配方式
db.option.matchType=匹配方式
#非递归
db.text.nonRecursion=非递归
#递归
db.text.recursion=递归
#设置
db.title.setting=设置
#不改变文件位置
db.text.noChangeDir=不改变文件位置
#迁移后删除文件
db.text.transDel=迁移后删除文件
#迁移后将文件移动至
db.text.transMoveDir=迁移后将文件移动至
#请输入通配符(正则表达式),多个表达式用'|'分割)
db.text.enterInfo=请输入通配符(正则表达式),多个表达式用'|'分割)
#目标
db.text.target=目标
#目标文件名
db.option.tarDir=目标文件名
#使用源文件名
db.text.useSrcFilename=使用源文件名
#源文件名加上
db.text.srcFilenameAdd=源文件名加上
#导入日期
db.text.importDate=导入日期
#如
db.text.example=如
#导入时间
db.text.importTime=导入时间
#若文件重名
db.option.fileNameSame=若文件重名
#覆盖
db.text.override=覆盖
#追加
db.text.append=追加
#跳过
db.text.ignore=跳过
#重命名自动添加后缀
db.text.fileNameSuffix=重命名自动添加后缀
#文件匹配情况
db.text.matchFile=文件匹配情况
#未找到匹配的文件,请检查路径和正则表达式
db.text.matchNoneNote=未找到匹配的文件,请检查路径和正则表达式
#设置增量对比字段
db.title.setSynch=设置增量对比字段
#选择主键
db.option.choosePkName=选择主键
#源表字段
db.text.srcTblColumn=源表字段
#目标表字段
db.text.tarTblColumn=目标表字段
#存储格式
db.option.tableType=存储格式
#格式化
db.act.sqlFormat=SQL格式化
#查询SQL编辑框
db.title.sqlEditor=查询SQL编辑框
#仅显示前5条数据
db.text.querylimit5=仅显示前5条数据
#迁移数据预览结果
db.title.preViewResult=迁移数据预览结果
#SQL查询
db.option.sqlQuery=SQL查询
#字段映射和清洗
db.title.columnsMap=字段映射和清洗
#字段映射配置
db.option.columnsConfig=字段映射配置
#按字段顺序自动映射
db.option.mapIndex=按字段顺序自动映射
#按字段名称自动映射
db.option.mapName=按字段名称自动映射
#清空配置
db.option.emptyMap=清空配置
#类型
db.text.type=类型
#函数列表
db.title.funcList=函数列表
#数据内容处理
db.option.dataFunc=数据内容处理
#字符替换
db.text.Replace=字符替换
#替换换行符
db.text.ReplaceEnter=替换换行符
#去除空格
db.text.trim=去除空格
#字符长度截取
db.text.substr=字符长度截取
#剔除字符
db.text.delChar=剔除字符
#增加字段
db.option.addColumn=增加字段
#增加时间函数
db.text.timestamp=增加时间函数
#增加常值
db.text.const=增加常值
#默认返回NULL
db.text.null=默认返回NULL
#保存整个配置
db.act.saveConfig=保存整个配置
#函数名称
db.text.funcName=函数名称
#替换前
db.text.oldChar=替换前
#替换后
db.text.newChar=替换后
#添加需要替换的字符
db.act.oldChar=添加需要替换的字符
#去除首尾空格
db.option.trimBlank=去除首尾空格
#去除所有空格
db.option.trimAll=去除所有空格
#正序
db.text.order=正序
#逆序
db.text.reverse=逆序
#个字符开始截取
db.text.charSub=个字符开始截取
#截取长度
db.text.subLen=截取长度
#不限制
db.text.nolimit=不限制
#选择类型
db.text.selectType=选择类型
#时间戳
db.text.stamp=时间戳
#目标数据
db.text.tarData=目标数据
#待映射列表
db.text.mapTblList=待映射列表
#忽略
db.text.skip=忽略
#中断
db.text.interrupt=中断
#过滤函数
db.text.filterFunc=过滤函数
#SQL文件
dump.title.title=SQL文件                        
#管理SQL文件	
dump.title.version=管理SQL文件
#请输入备份名关键字进行查询	
dump.option.search=请输入备份名关键字进行查询
#备份日期	
dump.title.verDate=备份日期
#备份名	
dump.option.verName=备份名
#表数量	
dump.tblNum=表数量
#操作日期	
dump.option.opDate=操作日期
#上传方式	
dump.upType=上传方式
#上传用户	
dump.option.upUser=上传用户
#查看备份SQL文件	
dump.title.verSearch=查看备份SQL文件
#备份信息	
dump.verInfo=备份信息
#备份名	
dump.option.verNum=备份名
#备份创建时间	
dump.option.verNewTime=备份创建时间
#文件总大小	
dump.option.fileSize=文件总大小
#文件数	
dump.option.fileNum=文件数
#目标类型	
dump.option.tarType=目标类型
#至
dump.text.to=至
#浏览器
dump.text.browser=浏览器
#客户端
dump.text.client=客户端
#的基本信息
dump.text.info=的基本信息
#目标表SQL语句
dump.text.tarSql=目标表SQL语句
#备份数据表	
dump.title.verTbls=备份数据表
#所导入的数据表	
importedTbls=所导入的数据表
#备份地址	
dump.option.bakAddr=备份地址
#只显示前100条记录	
dump.option.show100=只显示前100条记录
#SQL文件迁移	
dump.title.dumpMigrate=SQL文件迁移
#上传SQL文件	
dump.title.upDumpFiles=上传SQL文件
#配置导入路径	
dump.title.importPath=配置导入路径
#添加SQL文件	
dump.option.addDump=添加文件
#(注：仅支持insert文本文件内容,及*.sql,*.txt,*.gz,*.zip的格式解析;且一次任务最多同时上传5个SQL文件。)	
dump.note.dumpNote=(注：仅支持insert文本文件内容,及*.sql,*.txt,*.gz,*.zip的格式解析;且一次任务最多同时上传5个SQL文件。)
#批量配置	
dump.title.batchConfig=批量配置
#对此次上传的SQL文件进行备份	
dump.option.isBak=对此次上传的SQL文件进行备份
#备份后,所上传的SQL文件会额外保存在备份库中,并能在SQL备份管理中查看相关信息及预览其中部分信息内容。	
dump.note.dumpBakNote=备份后,所上传的SQL文件会额外保存在备份库中,并能在SQL备份管理中查看相关信息及预览其中部分信息内容。
#请选择需要备份的路径	
dump.option.bakPath=请选择需要备份的路径
#提示
dump.option.tip=提示
#您本次的导入操作中,有
dump.text.importAct=您本次的导入操作中,有
#张表在目标位置不存在,系统将默认为您进行建表操作。若新建序列存在相同目标库、目标表则默认新建第一个。	
dump.text.newTblNote=张表在目标位置不存在,系统将默认为您进行建表操作。若新建序列存在相同目标库、目标表则默认新建第一个。
#新建表如下
dump.text.newTblList=新建表如下
#以下新表创建失败：
dump.text.newTblFail=以下新表创建失败
#导入
dump.text.import=导入
#源库名	
dump.option.sourceDb=源库名
#目标表名	
dump.option.tarTbl=目标表名
#已了解,继续下一步	
dump.note.nextStep=已了解,继续下一步
#取消,我要修改	
dump.note.cancel=取消,我要修改
#所有任务数据配置完毕！
dump.note.configOk=所有任务数据配置完毕!
#所有目标表验证完毕！
dump.note.tblsNew=所有目标表验证完毕!
#亚马逊S3数据迁移
s3.title.s3Migrate=亚马逊S3数据迁移
#配置连接参数
s3.title.configLink=配置连接参数
#选择表及配置目标路径
s3.title.configParam=选择表及配置目标路径
#请提供Amazon S3的Access Key（访问密钥）和Secret Key（秘密密钥）
s3.option.key=请提供Amazon S3的Access Key（访问密钥）和Secret Key（秘密密钥）
#访问密钥ID
s3.option.keyId=访问密钥ID
#私有访问密钥
s3.option.accKey=私有访问密钥
#区域
s3.option.region=区域
#注意：Amazon S3的文件目前只支持导入到HDFS
s3.note.hdfsNote=注意：Amazon S3的文件目前只支持导入到HDFS
#已选择
s3.option.selected=已选择
#所有任务数据配置完毕！
s3.note.configed=所有任务数据配置完毕！
#开始任务
s3.note.start=开始任务
#请求资源为空,请重新加载
s3.info.resEmpty=请求资源为空,请重新加载
#请求资源为空
s3.info.resErr=请求资源为空
#迁移
s3.title.migrate=迁移
#注意：FTP目前只支持导入到HDFS
ftp.note.hdfsNote=注意：FTP目前只支持导入到HDFS
#注：根据您的配置,可对目标位置已有的同名文件进行替换。
ftp.note.importNote=注：根据您的配置, 可对目标位置已有的同名文件进行替换
#文件夹名
ftp.option.dirTitle=文件夹名
#返回根目录
ftp.option.returnRoot=返回根目录
#本地文件迁移
local.title.localMigrate=本地文件迁移
#配置目标路径
local.title.tarPath=配置目标路径
#目标位置
local.title.tarDir=目标位置
#添加文件
local.option.addFile=添加文件
#(注：最多批量上传5个文件;文件大于500M, 建议使用客户端上传.若目标位置存在跟迁移对象同名的文件，系统默认对迁移成功的文件名增加后缀处理。(例：测试文件_1、测试文件_2))
local.note.limit500=（注：最多批量上传5个文件；文件大于500M，建议使用客户端上传。若目标位置存在跟迁移对象同名的文件，系统默认对迁移成功的文件名增加后缀处理。（例：测试文件_1、测试文件_2））
#注：若目标位置存在跟迁移对象同名的文件，系统默认对迁移成功的文件名增加后缀处理。(例：测试文件_1、测试文件_2)
csv.text.hdfsNote=注：若目标位置存在跟迁移对象同名的文件，系统默认对迁移成功的文件名增加后缀处理。(例：测试文件_1、测试文件_2)
#文件大小
local.option.fileSize=文件大小
#上传
local.option.up=上传
#路径配置
local.option.confPath=路径配置
#解析
local.option.parse=解析
#开始导入
local.title.startTask=开始导入
#文件目录
local.title.fileDir=文件目录
#文件名
local.title.fileName=文件名
#CSV迁移
local.title.csvMigrate=CSV迁移
#EXCEL迁移
local.title.excelMigrate=EXCEL迁移
#配置目标位置
local.title.configTar=配置目标位置
#(注: 支持CSV和TXT文件, 最大800M, 最多5个文件批量上传)
local.note.limit800=(注: 支持CSV和TXT文件, 最大800M, 最多5个文件批量上传)
#数据源
local.title.dataSrc=数据源
#导入配置
local.title.importConf=导入配置
#HDFS目录
local.title.hdfsDir=HDFS目录
#浏览
local.option.scan=浏览
#从第
local.text.from=从第
#行开始导入数据,之前的数据将不做保留
local.note.saveLineNum=行开始导入数据,之前的数据将不做保留
#目标SQL语句
local.option.tarSql=目标SQL语句
#若您填写的表名在目标位置不存在,系统会默认通过SQL语句新建表以导入数据。若建表SQL有误,将影响数据的导入。请您检查目标SQL语句并根据个人需求进行修改。
local.note.newTblNote=若您填写的表名在目标位置不存在,系统会默认通过SQL语句新建表以导入数据。若建表SQL有误,将影响数据的导入。请您检查目标SQL语句并根据个人需求进行修改。
#CSV文件参数
local.title.csvConf=CSV文件参数
#制表符
local.option.tab=制表符
#分号
local.option.semicolon=分号
#逗号
local.option.comma=逗号
#空格
local.option.space=空格
#其他
local.option.other=其他
#(注: 支持后缀为.xls和.xlsx的Excel文件, 最大100M, 最多5个文件批量上传)
local.note.excLimit100=(注: 支持后缀为.xls和.xlsx的Excel文件, 最大100M, 最多5个文件批量上传)
#库
local.option.db=库
#表
local.option.tbl=表
#分区
local.option.part=分区
#列分隔符
local.option.columnSep=列分隔符
#文本限定符设置
local.option.endSet=文本限定符设置
#文本限定符
local.option.endChar=文本限定符
#保留
local.option.stay=保留
#去掉后导入
local.option.quit=去掉后导入


#客户端任务管理
clientM.title.clientTaskM=客户端任务管理
#客户端管理
clientM.title.clientM=客户端管理
#任务查看
clientM.title.taskView=任务查看
#新建任务
clientM.title.newTask=新建任务
#天调度类型
clientM.option.dayExec=天调度类型
#周调度类型
clientM.option.weekExec=周调度类型
#月调度类型
clientM.option.monthExec=月调度类型
#小时调度类型
clientM.option.hourExec=小时调度类型
#定时执行
clientM.option.timing=定时执行
#立即执行
clientM.option.immedi=立即执行
#任务说明
clientM.option.taskNote=任务说明
#调度生效日期
clientM.cronLife=调度生效日期
#无
clientM.option.none=无
#客户端列表
clientList.title.clientList=客户端列表
#客户端上传
clientList.title.clientUp=客户端管理
#下载客户端
clientList.title.downClient=下载客户端
#同步客户端
clientList.title.synch=同步客户端
#本地文件数据轻松管理,自动同步本地资源、文件、SQL文件等至设置好的目标位置,告别频繁上传、文件管理不再愁。
clientList.note.clientDesc=本地文件数据轻松管理,自动同步本地资源、文件、SQL文件等至设置好的目标位置,告别频繁上传、文件管理不再愁。
#备注
clientList.note.mark=备注
#一个IP地址只能启动一个客户端
clientList.note.ip2client=一个IP地址只能启动一个客户端
#客户端成功部署以后,将进入客户端管理列表
clientList.note.accList=客户端成功部署以后,将进入客户端管理列表
#下载新的客户端
clientList.option.downNewClient=下载新的客户端
#客户端成功部署以后,在客户端列表可以看到您的客户端信息
clientList.note.clientNews=客户端成功部署以后,在客户端列表可以看到您的客户端信息
#客户端名
clientList.option.clientName=客户端名
#服务地址
clientList.option.ip=服务地址
#操作
clientList.option.oper=操作
#任务
clientList.option.task=任务
#任务名
clientList.option.taskName=任务名
#状态
clientList.option.status=状态
#删除
clientList.option.del=删除
#删除客户端后,该客户端所有任务将被停止,并且在客户端列表中清除该客户端所有信息。
clientList.note.delClientNote=删除客户端后,该客户端所有任务将被停止,并且在客户端列表中清除该客户端所有信息。
#确定将该客户端删除？
clientList.note.isDel=确定将该客户端删除？
#迁移任务首页
index.title.index=迁移任务首页
#数据迁移
index.title.dataMigrate=数据迁移
#任务管理
index.title.taskManage=任务管理
#新建迁移任务
index.title.newTask=新建迁移任务
#任务布置成功
index.title.succPage=任务布置成功
#您是要
index.text.will=您是要
#进行查看
index.text.view=进行查看
#还是前往
index.text.goto=还是前往
#全部
index.title.all=全部
#本地文件
index.title.local=本地文件
#数据库
index.title.db=数据库
#第三方平台
index.title.otherPlat=第三方平台
#其他
index.title.other=其他
#本地到HDFS
index.title.local2Hdfs=本地到HDFS
#亚马逊
index.title.amazon=亚马逊
#阿里云
index.title.ali=阿里云
#返回导航页
common.title.retIndex=返回导航页
#aside 数据迁移
common.title.dataMigrate=数据迁移
#总览
common.title.overview=总览
#退出
common.title.exit=退出
#语言项
common.option.lang=English
#权限管理
common.title.auth=权限管理
#数据迁移
common.title.migrate=数据迁移
#数据采集
common.title.collect=数据采集
#数据下载
common.title.down=数据下载
#迁移任务管理
common.title.migrateM=迁移任务管理
#新建迁移任务
common.title.newTask=新建迁移任务
#任务执行历史记录
common.title.runLog=任务执行历史记录
#迁移运行实例
common.title.migrateEg=迁移运行实例
#设置Settings
common.title.setting=设置
#资源库信息管理
common.title.resM=资源库信息管理
#环境配置
common.title.envir=环境配置
#编辑
common.title.edit=编辑
#任务详情
common.option.taskDetail=任务详情
#报警服务
common.title.alarmService=报警服务
#报警历史
common.option.alarmRecord=报警历史
#报警规则
common.option.alarmSetting=报警规则
#流式计算
common.title.pipeline=流式计算
#开放API
common.tab.openAPI=开放API
common.title.openAPI=开放API
#Token管理
common.title.token=Token管理
#API说明文档
common.title.api=API说明文档

#数据库连接管理
dbManage.title.dbLink=数据库连接信息
#Hive连接信息
dbManage.title.hiveLink=Hive连接信息
#Hdfs连接信息
dbManage.title.hdfsLink=Hdfs连接信息
#Ftp连接信息
dbManage.title.ftpLink=Ftp连接信息
#Spark MPP连接信息
dbManage.title.sparkLink=Spark MPP连接信息
#协议
dbManage.option.protocol=协议
#文件传输协议
dbManage.option.ftp=文件传输协议
#新建资源库信息
dbManage.title.newRes=新建资源库信息
#资源信息修改
dbManage.title.resEdit=资源信息修改
#基础
dbManage.title.base=基础
#参数配置
dbManage.title.param=参数配置
#请选择项目
dbManage.title.setApp=请选择项目
#资源库类型
dbManage.title.setResType=资源库类型
#服务地址
dbManage.option.addr=服务地址
#认证方式
dbManage.title.authType=认证方式
#路径
dbManage.option.path=路径
#文件路径
dbManage.option.fileName=文件路径
#用户名
dbManage.option.userName=用户名
#连接名
dbManage.option.connName=连接名
#保存
dbManage.option.save=保存
#连接
dbManage.option.link=连接 
#代理用户
dbManage.option.proxy=代理用户
#端口
dbManage.option.port=端口
#数据库名
dbManage.option.dbName=数据库名
#密码
dbManage.option.pwd=密码
#所属项目
dbManage.option.belongApp=所属项目
#数据库类型
dbManage.option.dbType=数据库类型
#上传文件
dbManage.title.upFile=上传文件
#上传文件btn
dbManage.button.upFile=上传文件
#Kerberos域
dbManage.title.kerberos=Kerberos域
#自定义连接名
dbManage.title.linkName=自定义连接名
#绑定的HDFS连接
dbManage.title.setHdfsLink=绑定的HDFS连接
#HIVE代理用户
dbManage.title.hiveUser=HIVE代理用户
#选择功能参数
dbManage.title.setParam=选择功能参数
#命名参数
dbManage.title.nameParam=命名参数
#值
dbManage.title.val=值
#测试连接
dbManage.title.testConn=测试连接
#新增一行
dbManage.option.newline=新增一行
#请输入服务地址
dbManage.enter.host=请输入服务地址
#请输入数据库名
dbManage.enter.dbName=请输入数据库名
#请输入用户名
dbManage.enter.userName=请输入用户名
#请输入密码
dbManage.enter.password=请输入密码
#请自定义连接名,默认为"服务地址_端口_数据库名"
dbManage.enter.connName=请自定义连接名,默认为'服务地址_端口_数据库名'
#请填写与上传keytab文件对应的keberos域
dbManage.enter.principal=请填写与上传keytab文件对应的keberos域
#请自定义连接名,默认为："hdfs_认证方式"
dbManage.enter.hdfsConnName=请自定义连接名,默认为：'hdfs_认证方式'
#请输入端口
dbManage.enter.port=请输入端口
#请输入代理用户
dbManage.enter.hiveUserName=请输入代理用户
#请自定义连接名,默认为："协议_服务地址_端口"
dbManage.enter.hiveConnName=请自定义连接名,默认为：'协议_服务地址_端口'

#同一项目内的成员将共同管理项目内的资源库连接
dbManage.tip.app=同一项目内的成员将共同管理项目内的资源库连接
#资源库包含数据库、Hive、HDFS、FTP、Spark MPP类型。创建Hive连接时,请确保已创建同一集群的HDFS连接
dbManage.tip.type=资源库包含数据库、Hive、HDFS、FTP、Spark MPP类型。创建Hive连接时,请确保已创建同一集群的HDFS连接
#连接名将作为数据源的标识
dbManage.tip.connName=连接名将作为数据源的标识
#选择认证方式时,请确保要添加的集群已集成相对应的权限管理
dbManage.tip.authType=选择认证方式时,请确保要添加的集群已集成相对应的权限管理
#core-site.xml是Hadoop集群的配置文件,通常在Hadoop安装目录内,如路径../conf/core-site.xml,请上传
dbManage.tip.coreSite=core-site.xml是Hadoop集群的配置文件,通常在Hadoop安装目录内,如路径../conf/core-site.xml,请上传
#hdfs-site.xml是Hadoop集群的配置文件,通常在Hadoop安装目录内,如路径../conf/hdfs-site.xml,请上传
dbManage.tip.hdfsSite=hdfs-site.xml是Hadoop集群的配置文件,通常在Hadoop安装目录内,如路径../conf/hdfs-site.xml,请上传
#HDFS用户名与该集群的HDFS目录权限有关
dbManage.tip.hdfsUser=HDFS用户名与该集群的HDFS目录权限有关
#krb5.conf是hadoop集群kerberos认证方式的配置文件,通常所在位置如/etc/krb5.conf,请上传
dbManage.tip.krb=krb5.conf是hadoop集群kerberos认证方式的配置文件,通常所在位置如/etc/krb5.conf,请上传
#keytab文件是hadoop集群kerberos认证方式的配置文件,通常所在位置如/etc/security/keytabs/datahub.keytab,请上传
dbManage.tip.keytab=keytab文件是hadoop集群kerberos认证方式的配置文件,通常所在位置如/etc/security/keytabs/datahub.keytab,请上传
#hadoop集群的Kerberos域名,如datahub@LEAP.COM
dbManage.tip.kerberos=hadoop集群的Kerberos域名,如datahub@LEAP.COM
#hive连接需绑定同一集群的HDFS连接
dbManage.tip.bindHdfs=hive连接需绑定同一集群的HDFS连接
#服务地址,如demo1.test.com
dbManage.tip.host=服务地址,如demo1.test.com
#连接的端口
dbManage.tip.port=连接的端口
#hive库表权限依赖于此代理用户,请在集群内创建相应的目录,并赋予此代理用户。如代理用户为“demo”,请在集群HDFS内创建目录“/user/demo”,且此目录属于用户“demo”
dbManage.tip.proxy=hive库表权限依赖于此代理用户,请在集群内创建相应的目录,并赋予此代理用户。如代理用户为“demo”,请在集群内创建目录“/user/demo”,且此目录属于用户“demo”
#连接URL会按照服务地址等填写信息自动生成,若集群为kerberos认证方式,请对连接串内principal的值进行修改。principal值是hive配置文件hive-site.xml中hive.metastore.kerberos.principal值
dbManage.tip.url=连接URL会按照服务地址等填写信息自动生成,若集群为kerberos认证方式,请对连接串内principal的值进行修改。principal值是hive配置文件hive-site.xml中hive.metastore.kerberos.principal值
#服务模式
dbManage.option.mode=服务模式
#普通模式
dbManage.option.normalMode=普通模式
#HA模式
dbManage.option.haMode=HA模式
#ZK节点
dbManage.option.zk=ZK节点
#添加
dbManage.option.add=添加

#当前集成了hadoop集群的两种认证方式（sentry,kerberos）。选择认证方式时,请确保部署集群已集成相对应的权限管理
envir.tip.sentry=当前集成了hadoop集群的两种认证方式（sentry,kerberos）。选择认证方式时,请确保部署集群已集成相对应的权限管理
#core-site.xml为hadoop集群的配置文件,通常所在位置为：/etc/hadoop/conf/core-site.xml
envir.tip.coreSite=core-site.xml为hadoop集群的配置文件,通常所在位置为：/etc/hadoop/conf/core-site.xml
#hdfs-site.xml为hadoop集群的配置文件,通常所在位置为：/etc/hadoop/conf/hdfs-site.xml
envir.tip.hdfsSite=hdfs-site.xml为hadoop集群的配置文件,通常所在位置为：/etc/hadoop/conf/hdfs-site.xml
#krb5.conf为hadoop集群kerberos认证方式的配置文件,通常所在位置为：/etc/krb5.conf
envir.tip.krb5Path=krb5.conf为hadoop集群kerberos认证方式的配置文件,通常所在位置为：/etc/krb5.conf
#部署集群为kerberos认证方式时,通常使用datahub.keytab进行配置,通常所在位置为：/etc/security/keytabs/datahub.keytab。使用datahub.keytab前,请确保/user/datahub目录已存在,并此目录属于datahub用户
envir.tip.keytabPath=部署集群为kerberos认证方式时,通常使用datahub.keytab进行配置,通常所在位置为：/etc/security/keytabs/datahub.keytab。使用datahub.keytab前,请确保/user/datahub目录已存在,并此目录属于datahub用户
#hadoop集群Kerberos认证方式下,keytab的principal值,如datahub@LEAP.COM
envir.tip.kerberos=hadoop集群Kerberos认证方式下,keytab的principal值,如datahub@LEAP.COM
#datahub所部署集群的HDFS连接名默认为LEAP_SYSTEM_HDFS
envir.tip.hdfsName=datahub所部署集群的HDFS连接名默认为LEAP_SYSTEM_HDFS
#datahub所部署集群的Hive jdbc连接URL
envir.tip.hid=datahub所部署集群的Hive jdbc连接URL
#代理用户与Hive库表权限有关
envir.tip.proxy=代理用户与Hive库表权限有关
#datahub所部署集群的HIVE连接名默认为LEAP_SYSTEM_HIVE
envir.tip.hiveName=datahub所部署集群的HIVE连接名默认为LEAP_SYSTEM_HIVE

#项目环境配置
envir.tab.envir=项目环境配置
#运营商服务配置
envir.tab.sms=运营商服务配置
#流式计算配置
envir.tab.streamEntry=流式计算配置
#测试
envir.act.test=测试
#接收号码参数名
envir.option.phoneNumberKey=接收号码参数名
#短信内容参数名
envir.option.contentKey=短信内容参数名
#其他参数
envir.option.params=其他参数
#添加参数
envir.option.addParam=添加参数
#输入手机号
envir.text.enterPhone=输入手机号
#案例
envir.text.case=案例

#迁移任务查询
manage.title.taskSearch=迁移任务查询
#任务ID
manage.title.taskId=任务ID
#任务名称
manage.title.taskName=任务名称
#请输入任务名称
manage.text.inputTaskName=请输入任务名称
#任务状态
manage.title.taskStatus=任务状态
#所有
manage.title.all=所有
#执行中
manage.title.execing=执行中
#结束
manage.title.end=结束
#暂停
manage.title.pause=暂停
#任务执行时间
manage.title.execTime=任务执行时间
#创建人
manage.title.owner=创建人
#我创建的
manage.title.myPro=我创建的
#所属项目
manage.title.inApp=所属项目
#全部项目
manage.title.allPro=全部项目
#查询
manage.title.search=查询
#重置
manage.title.reset=重置
#任务描述
manage.title.taskDesc=任务描述
#源类型
manage.title.srcType=源类型
#调度类型
manage.title.cronType=调度类型
#任务创建时间
manage.title.createTime=任务创建时间
#暂无数据
manage.title.noData=暂无数据
#停用
manage.title.stop=停用
#激活
manage.title.enable=激活
#修改
manage.title.update=修改
#复制
manage.title.copy=复制
conf.btn.copy=复制
#运行记录
manage.title.runLog=运行记录
#执行历史查询
manage.title.hisSearch=执行历史查询
#运行中
manage.title.running=运行中
#成功
manage.title.succ=成功
#失败
manage.title.fail=失败
#执行日期
manage.title.execDate=执行日期
#开始时间
manage.title.startTime=开始时间
#结束时间
manage.title.endTime=结束时间
#运行时长
manage.title.runTime=运行时长
#日志
manage.title.log=日志
#下载失败明细
manage.act.failItems=下载失败明细
#客户端任务
client.title.clientTask=客户端任务
#配置源
client.title.confSrc=配置源
#配置导入路径
client.title.confUpPath=配置导入路径
#导入规则设置
client.title.confRule=导入规则设置
#指定源文件路径
client.option.srcPath=指定源文件路径
#填写源文件所在路径
client.title.inputSrcPath=填写源文件所在路径
#是否上传子目录文件
client.title.isUpdir=是否上传子目录文件
#过滤上传文件
client.title.filterFile=过滤上传文件
#勾选此项后,可对指定需要上传的源位置进行文件类型过滤。
client.note.filterTip=勾选此项后,可对指定需要上传的源位置进行文件类型过滤。
#文件类型过滤
client.option.filterType=文件类型过滤
#仅上传指定类型文件
client.title.upType=仅上传指定类型文件
#不上传指定类型文件
client.title.noUpType=不上传指定类型文件
#文件类型选择
client.option.selectType=文件类型选择
#其他类型
client.title.otherType=其他类型
#若要过滤的文件类型不在以上备选框内,请在下面填写文件类型的正则表达式并以“|”隔开,例如：.+\.sh|.+\.xml
client.note.filterNote=若要过滤的文件类型不在以上备选框内,请在下面填写文件类型的正则表达式并以“|”隔开,例如：.+\\.sh|.+\\.xml
#已上传文件识别设置
client.title.upedReg=已上传文件识别设置
#客户端上传任务需要对已上传过的文件进行识别,以免进行重复上传,您可以选择将已上传过的文件自动增加后缀标识,或者移动到其他的文件夹,系统会自动根据您的设置来完成操作。
client.note.upedRegNote=客户端上传任务需要对已上传过的文件进行识别,以免进行重复上传,您可以选择将已上传过的文件自动增加后缀标识,或者移动到其他的文件夹,系统会自动根据您的设置来完成操作。
#选择处理方式
client.option.dealType=选择处理方式
#删除本地已上传的文件
client.option.delUped=删除本地已上传的文件
#为已上传文件添加后缀
client.option.addSuff=为已上传文件添加后缀
#将已上传的文件移动到
client.option.upedMove=将已上传的文件移动到
#取消
client.option.cancel=取消
#上一步
client.option.prevStep=上一步
#下一步
client.option.nextStep=下一步
#选择连接
client.option.selectLink=选择连接
#填写目标路径
client.option.tarDir=填写目标路径
#选择目标数据库
client.option.tarDb=选择目标数据库
#选择目标数据表
client.option.tarTbl=选择目标数据表
#选择分区
client.option.part=选择分区
#若指定的目标表与待上传的表重名,则
client.note.sameNameTbl=若指定的目标表与待上传的表重名,则：
#覆盖目标表数据
client.option.override=覆盖目标表数据
#向目标表追加数据
client.option.append=向目标表追加数据
#配置任务信息
client.title.confTask=配置任务信息
#任务名称长度不能超过30个字符。
client.info.nameLen=任务名称长度不能超过30个字符。
#最长255个字符
client.info.maxLen=最长255个字符
#执行一次
client.option.once=执行一次
#重复执行
client.option.regExec=重复执行
#定时执行
client.option.timing=定时执行
#日期
client.option.date=日期
#时间
client.option.time=时间
#周期
client.option.cycle=周期
#执行时间
client.option.execTime=执行时间
#每小时
client.option.perHour=每小时
#每天
client.option.perDay=每天
#每周
client.option.perWeek=每周
#每月
client.option.perMonth=每月
#第
client.text.first=第
#号
client.option.dayNum=号
#分钟
client.option.minNum=分钟
#管理
client.option.manage=管理
#点
client.text.hour=点
#星期一
client.option.Mon=星期一
#星期二
client.option.Tue=星期二
#星期三
client.option.wed=星期三
#星期四
client.option.thu=星期四
#星期五
client.option.fri=星期五
#星期六
client.option.sat=星期六
#星期日
client.option.sun=星期日
#每月最后一天
client.option.lastDay=每月最后一天
#生效时间
client.option.effectTime=生效时间
#无结束时间
client.option.Endless=无结束时间
#摘要
client.option.summary=摘要
#说明
client.option.note=说明
#根据完整的参数,自动生成说明
client.note.note=根据完整的参数,自动生成说明
#立刻执行任务
client.option.Immed=立即执行
#开始导入
client.option.startImport=开始导入
#HDFS配置路径
client.option.hdfsPath=HDFS配置路径
#确定
client.option.sure=确定
#请确保上传文件和目标表分隔符一致
client.text.columnTip=请确保上传文件和目标表分隔符一致
#文件过滤设置
client.option.fileFilter=文件过滤设置
#不过滤上传文件
client.option.filterNone=不过滤上传文件
#迁移设置
client.option.transSetting=迁移设置
#源文件中列分隔符
client.option.srcColumnSep=源文件中列分隔符
#一次性导入
cron.title.once=一次性导入
#周期性自动导入
cron.title.cycle=周期性自动导入
#关闭日期框
cron.option.closeCal=关闭日期框
#AWZ regions
us-gov-west-1=★  美国西部 (政府)
ap-northeast-1=亚太区域 (东京)
ap-northeast-2=亚太区域 (首尔)
ap-south-1=亚太区域 (孟买)
ap-southeast-1=亚太区域 (新加坡)
ap-southeast-2=亚太区域 (悉尼)
ca-central-1=加拿大 (中部)
eu-central-1=欧洲 (法兰克福)
eu-west-1=欧洲 (爱尔兰)
eu-west-2=欧洲 (伦敦)
sa-east-1=南美洲 (圣保罗)
us-east-1=美国东部 (弗吉尼亚北部)
us-east-2=美国东部 (俄亥俄州)
us-west-1=美国西部 (加利福尼亚北部)
us-west-2=美国西部 (俄勒冈)
cn-north-1=★  中国北部 (北京)

# 提示信息 start
# MSG.T类型编号.E错误信息编号
# MSG.T类型编号.I提示信息编号
MSG.T0000.E000000=系统错误
MSG.T0000.E000001=系统错误: 系统内部错误, 请联系管理员
MSG.T0000.E000002=系统错误: 域名解析异常, 未知域名 %s
MSG.T0001.E000000=无效参数值
MSG.T0001.E000001=无效参数值: %s不能为空
MSG.T0001.E000002=无效参数值: %s不能为负数
MSG.T0001.E000003=无效参数值: 参数%s格式错误
MSG.T0001.E000004=无效参数值: 参数%s数量为%s, 参数%s数量为%s, 数量不相等
MSG.T0001.E000005=无效参数值: 参数%s数量不能为0
MSG.T0001.E000006=无效参数值: 参数%s最大长度为%s, 最小长度为%s
MSG.T0001.E000007=无效参数值: 参数%s只能是数字
MSG.T0001.E000008=无效参数值: 参数%s最小值为'%s', 但是输入为 '%s'
MSG.T0001.E000009=无效参数值: 参数%s最大值为'%s', 但是输入为 '%s'
MSG.T0001.E000010=无效参数值: 两个参数不能同时为空, %s和%s不能同时为空
MSG.T0001.E000011=无效参数值: Hive配置未绑定有效的HDFS连接, 请检查Hive配置
MSG.T0001.E000012=无效参数值: 参数%s位数不等于%s
MSG.T0001.E000013=无效参数值: 已超过最大支持数量: 单个任务数据源表数量(%s)已超过最大支持数, 最大值=%s, 当前值=%s
MSG.T0001.E000014=无效参数值: 参数%s无效, 非法的表名, 字母数字下划线组成
MSG.T0001.E000015=无效参数值: 参数%s无效, 建表语句语法错误, 以CREATE开始\n%s
MSG.T0001.E000016=无效参数值: 参数%s无效, 建表语句语法错误, CREATE附近错误\n%s
MSG.T0001.E000017=无效参数值: 参数%s无效, 建表语句语法错误, TABLE附近错误\n%s
MSG.T0001.E000018=无效参数值: 参数%s无效, 输入的表名[%s]和建表语句DDL表名[%s]不一致
MSG.T0001.E000019=无效参数值: 参数%s无效, 正则表达式格式错误'%s'
MSG.T0001.E000020=无效参数值: 参数%s无效, 亚马逊S3区域不存在 '%s'
MSG.T0001.E000021=无效参数值: 参数不一致, %s和%s不一致, host='%s', url='%s'
MSG.T0002.E000000=环境配置错误
MSG.T0002.E000001=环境配置错误: LEAP系统hive连接URL有更新, 请配置最新的URL信息\nnew=%s\nold=%s
MSG.T0002.E000002=环境配置错误: LEAP Hadoop集群kerberos认证失败, 请检查配置
MSG.T0002.E000003=环境配置错误: 文件不存在 %s
MSG.T0002.E000004=环境配置错误: krb5.conf文件内容无效, 文件存在, 且文件大小不超过10M, 且包含内容"[libdefaults]", "[realms]", "[domain_realm]"\n%s
MSG.T0002.E000005=环境配置错误: LEAP HDFS未授权目录[%s], 请先创建目录
MSG.T0002.E000006=环境配置错误: Kerberos认证模式下, Hive URL未指定principal参数\n%s
MSG.T0002.E000007=环境配置错误: Hive JDBC URL连接串HA模式缺少"namespace.hive"参数
MSG.T0002.E000008=环境配置错误: URL格式错误: %s
MSG.T0002.E000009=环境配置错误: 保存SMS服务信息错误, 请检查配置
MSG.T0002.E000010=环境配置错误: 发送短信错误, 请检查配置
MSG.T0003.E000000=DB连接错误
MSG.T0003.E000001=DB连接错误: 连接名已存在 %s ,请修改为其它值
MSG.T0003.E000002=DB连接错误: 用户名或密码错误
MSG.T0003.E000003=DB连接错误: 未知的数据库 %s
MSG.T0003.E000004=DB连接错误: 通过端口[%s]连接到[%s]的TCP/IP连接失败。错误：拒绝连接
MSG.T0003.E000005=DB连接错误: 不支持该数据库类型 %s
MSG.T0003.E000006=DB连接错误: 无效的JDBC URL '%s' 连接到%s
MSG.T0004.E000000=创建表错误
MSG.T0004.E000001=创建表错误: 创建表失败\n%s
MSG.T0004.E000002=创建表错误: 抱歉，您没有该库%s的建表权限
MSG.T0004.E000003=创建表错误: 数量不等, 处理结果数量为%s, 目标表数量为%s
MSG.T0004.E000004=创建表错误: 
MSG.T0004.E000005=创建表错误: 
MSG.T0005.E000000=获取数据库错误
MSG.T0005.E000001=获取数据库错误: 
MSG.T0005.E000002=获取数据库错误: 
MSG.T0005.E000003=获取数据库错误: 
MSG.T0005.E000004=获取数据库错误: 
MSG.T0005.E000005=获取数据库错误: 
MSG.T0006.E000000=获取表错误
MSG.T0006.E000001=获取表错误: 
MSG.T0006.E000002=获取表错误: 
MSG.T0006.E000003=获取表错误: 
MSG.T0006.E000004=获取表错误: 
MSG.T0006.E000005=获取表错误: 
MSG.T0007.E000000=获取字段列错误
MSG.T0007.E000001=获取字段列错误: 
MSG.T0007.E000002=获取字段列错误: 
MSG.T0007.E000003=获取字段列错误: 
MSG.T0007.E000004=获取字段列错误: 
MSG.T0007.E000005=获取字段列错误: 
MSG.T0008.E000000=获取DDL错误
MSG.T0008.E000001=获取DDL错误: 源表%s, 目标表%s
MSG.T0008.E000002=获取DDL错误: 数量不等, 源表数量为%s, 目标表数量为%s
MSG.T0008.E000003=获取DDL错误: 
MSG.T0008.E000004=获取DDL错误: 
MSG.T0008.E000005=获取DDL错误: 
MSG.T0008.E000006=获取DDL错误: 
MSG.T0009.E000000=获取分区错误
MSG.T0009.E000001=获取分区错误: 表不存在 %s
MSG.T0009.E000002=获取分区错误: 
MSG.T0009.E000003=获取分区错误: 
MSG.T0009.E000004=获取分区错误: 
MSG.T0009.E000005=获取分区错误: 
MSG.T0010.E000000=检索数据错误
MSG.T0010.E000001=检索数据错误: 
MSG.T0011.E000000=Hive连接错误
MSG.T0011.E000001=Hive连接错误: 
MSG.T0011.E000002=Hive连接错误: 
MSG.T0011.E000003=Hive连接错误: 
MSG.T0011.E000004=Hive连接错误: 
MSG.T0011.E000005=Hive连接错误: 
MSG.T0012.E000000=SparkMPP连接错误
MSG.T0012.E000001=SparkMPP连接错误: 
MSG.T0012.E000002=SparkMPP连接错误: 
MSG.T0012.E000003=SparkMPP连接错误: 
MSG.T0012.E000004=SparkMPP连接错误: 
MSG.T0012.E000005=SparkMPP连接错误: 
MSG.T0013.E000000=HDFS连接错误
MSG.T0013.E000001=HDFS连接错误: 连接名已存在 %s ,请修改为其它值
MSG.T0013.E000002=HDFS连接错误: krb5.conf文件内容无效, 文件存在, 且文件大小不超过10M, 且包含内容"[libdefaults]", "[realms]", "[domain_realm]"\n%s
MSG.T0013.E000003=HDFS连接错误: 尝试连接失败 %s
MSG.T0013.E000004=HDFS连接错误: 持久化HdfsInfo对象到数据库失败
MSG.T0013.E000005=HDFS连接错误: 权限不足, 拒绝访问: user=%s, access=READ_WRITE, owner=%s, path="%s"
MSG.T0013.E000006=HDFS连接错误: 
MSG.T0014.E000000=FTP连接错误
MSG.T0014.E000001=FTP连接错误: 获取FTP文件列表错误
MSG.T0014.E000002=FTP连接错误: 连接名已存在 %s ,请修改为其它值
MSG.T0014.E000003=FTP连接错误: 
MSG.T0014.E000004=FTP连接错误: 
MSG.T0014.E000005=FTP连接错误: 
MSG.T0015.E000000=S3连接错误
MSG.T0015.E000001=S3连接错误: 
MSG.T0016.E000000=客户端错误
MSG.T0016.E000001=客户端错误: 客户端未启动, 请启动客户端 [%s]
MSG.T0016.E000002=客户端错误: 
MSG.T0016.E000003=客户端错误: 
MSG.T0016.E000004=客户端错误: 
MSG.T0016.E000005=客户端错误: 
MSG.T0017.E000000=同一集群验证错误
MSG.T0017.E000001=同一集群验证错误: 该hdfs用户"%s"没有权限验证集群信息, 请确认hive和它绑定的hdfs是同一个集群
MSG.T0017.E000002=同一集群验证错误: 绑定的HDFS连接信息[%s]和Hive连接信息可能不是同一集群, 请检查配置\nHive访问地址%s 不在Hadoop集群列表中[\n%s]
MSG.T0017.E000003=同一集群验证错误: 
MSG.T0017.E000004=同一集群验证错误: 
MSG.T0017.E000005=同一集群验证错误: 
MSG.T0018.I000001=记录不存在: %s记录不存在[id=%s]
MSG.T0018.E000000=CSV文件上传错误
MSG.T0018.E000001=CSV文件上传错误: 上传文件中不包含任何*.txt|*.csv文件
MSG.T0018.E000002=CSV文件上传错误: 不支持的文件格式 [*.%s]
MSG.T0018.E000003=CSV文件上传错误: 
MSG.T0018.E000004=CSV文件上传错误: 
MSG.T0018.E000005=CSV文件上传错误: 
MSG.T0019.E000001=文件不存在: 文件不存在 %s
MSG.T0020.E000000=获取分隔符错误
MSG.T0021.E000000=SQL文件解析错误
MSG.T0021.E000001=SQL文件解析错误: SQL文件解析内容为空
MSG.T0022.E000000=上传错误
MSG.T0022.E000001=上传错误: hdfs文件加载到hive失败 %s
MSG.T0022.E000002=上传错误: 文件上传失败, 接收到的文件大小%s和实际大小%s不相等
MSG.T0023.E000000=文件合并错误
MSG.T0023.E000001=文件合并错误: 文件md5验证失败, 客户端md5=%s, 服务器md5=%s
MSG.T0024.E000000=登录错误
MSG.T0024.E000001=登录错误: 用户不存在
MSG.T0024.E000002=登录错误: 密码错误
MSG.T0025.E000000=重置错误
MSG.T0025.E000001=重置错误: 用户不存在
MSG.T0025.E000002=重置错误: 密码不能为空
MSG.T0026.E000000=注册错误
MSG.T0026.E000001=注册错误: 
MSG.T0027.E000000=任务保存错误
MSG.T0027.E000001=任务保存错误: 该任务正在执行中，请稍后重试[%s]
MSG.T0027.E000002=任务保存错误: 任务已过时\ncron=%s, \n有效时间 %s ~ %s
MSG.T0027.E000003=任务保存错误: 添加任务到调度器失败[%s]
MSG.T0027.E000004=任务保存错误: 文件不存在 %s
MSG.T0027.E000005=任务保存错误: 不支持的目标类型 %s
MSG.T0027.E000006=任务保存错误: 
MSG.T0027.E000007=任务保存错误: 
MSG.T0027.E000008=任务保存错误: 
MSG.T0027.E000009=任务保存错误: 
MSG.T0027.E000010=任务保存错误: 
MSG.T0028.E000000=任务调度错误
MSG.T0028.E000001=任务调度错误: 
MSG.T0029.I000001=激活任务: 任务 %s 已被停止
MSG.T0029.E000000=停止任务错误
MSG.T0029.E000001=停止任务错误: 该任务正在执行中，请先到执行历史停掉本次运行
MSG.T0030.I000001=激活任务: 任务 %s 已被激活
MSG.T0030.E000000=激活任务错误
MSG.T0030.E000001=激活任务错误: 
MSG.T0031.E000000=权限错误
MSG.T0031.E000001=权限错误: 您不是该项目成员, 操作无效
MSG.T0031.E000002=权限错误: 你不是系统管理员, 操作无效
MSG.T0031.E000003=权限错误: 请求被拒绝, 只接受post请求
MSG.T0031.E000004=权限错误: 权限拒绝, 文件或目录不能被删除, %s 不是以 %s 开始的
MSG.T0031.E000005=权限错误: 权限拒绝, 目录不能被删除 %s
MSG.T0032.E000000=删除错误
MSG.T0032.E000001=删除错误: LEAP系统配置不能删除
MSG.T0032.E000002=删除错误: 
MSG.T0033.E000000=下载错误
MSG.T0033.E000001=下载错误:
MSG.T0034.E000000=保存报警规则错误
MSG.T0035.E000000=Excel文件上传错误
MSG.T0035.E000001=Excel文件上传错误:
# 提示信息 end

# API提示信息 start
API.1001=token不存在
API.1002=token未被启用
API.1003=token已过期, 最终期限为 %s
API.1004=创建token的用户信息有误
API.1005=权限拒绝, token没有权限访问
API.1006=未开放的API '%s'
API.1007=连续访问次数过多, 请%s秒后重试
API.1008=token已失效, 请登录datahub web页面刷新token, 可能token用户已被移除项目'%s'
# API提示信息 end

#短信API测试内容
sms.test.content=Datahub短信服务API配置准备成功, 若非本人操作请忽略
#任务基本信息
conf.title.basicInfo=任务基本信息
#信息
conf.option.info=信息
#项
conf.option.item=项
#导入规则
conf.option.import=导入规则
#源信息
conf.title.sourceInfo=源信息
#目标信息
conf.title.tarInfo=目标信息
#迁移详情
conf.title.confDetail=迁移详情
#查看SQL
conf.act.viewSql=查看SQL
#查看任务执行记录
conf.button.viewLog=查看任务执行记录
#查看
conf.act.view=查看
#目标分区
conf.title.tarPart=目标分区
#源分区
conf.title.srcPart=源分区
#方式
conf.option.type=方式
#自定义分区
conf.option.customPart=自定义分区
#按源分区同步
conf.option.partSynch=按源分区同步
#若目标分区存在
conf.option.tarExist=若目标分区存在
#覆盖目标分区数据
conf.text.overridePart=覆盖目标分区数据
#向目标分区追加源分区数据
conf.text.appendPart=向目标分区追加源分区数据
#不要迁移
conf.text.ignorePart=不要迁移

#迁移任务报警
alarm.title.transAlarm=迁移任务报警
#客户端任务报警
alarm.title.clientAlarm=客户端任务报警
#客户端状态
alarm.option.clientStatus=客户端状态
#客户端任务
alarm.option.clientTask=客户端任务

#迁移任务
rule.title.transRule=迁移任务
#客户端任务
rule.title.clientRule=客户端任务
#新建规则
rule.act.newRule=新建规则
#新建报警规则
rule.title.newRule=新建报警规则
#修改报警规则
rule.title.editRule=修改报警规则
#报警类型
rule.option.ruleType=报警类型
#请选择
rule.option.tipSelect=请选择
#报警规划
rule.option.ruleSetting=报警规划
#任务失败
rule.type.taskFail=任务失败
#客户端离线
rule.option.clientOffline=客户端离线
#不限制报警次数
rule.option.alarmNolimit=不限制报警次数
#每小时报警一次
rule.option.alarmPerhour=每小时报警一次
#每3小时报警一次
rule.option.alarmPer3hour=每3小时报警一次
#每天报警一次
rule.option.alarmPerday=每天报警一次
#持续
rule.text.persist=持续
#个周期
rule.text.cycle=个周期
#报警内容
rule.option.alarmContent=报警内容
#报警接收方
rule.option.alarmUser=报警接收方
#短信
rule.option.sms=短信
#请输入联系人手机号，多个手机号用,隔开
rule.text.notifyUser=请输入联系人手机号，多个手机号用,隔开
#返回顶部
interface.act.toTop=返回顶部
#每个项目下每个用户对应一个独立的Token，用于API接口调用的验证，刷新操作可以更新Token。
token.text.tokenDesc=每个项目下每个用户对应一个独立的Token，用于API接口调用的验证，刷新操作可以更新Token。
